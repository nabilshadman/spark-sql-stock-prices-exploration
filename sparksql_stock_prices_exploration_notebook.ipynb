{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL Stock Prices Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bonjour le monde!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 + 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark application and Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on SparkContext in module pyspark.context object:\n",
      "\n",
      "class SparkContext(builtins.object)\n",
      " |  SparkContext(master=None, appName=None, sparkHome=None, pyFiles=None, environment=None, batchSize=0, serializer=PickleSerializer(), conf=None, gateway=None, jsc=None, profiler_cls=<class 'pyspark.profiler.BasicProfiler'>)\n",
      " |  \n",
      " |  Main entry point for Spark functionality. A SparkContext represents the\n",
      " |  connection to a Spark cluster, and can be used to create :class:`RDD` and\n",
      " |  broadcast variables on that cluster.\n",
      " |  \n",
      " |  .. note:: Only one :class:`SparkContext` should be active per JVM. You must `stop()`\n",
      " |      the active :class:`SparkContext` before creating a new one.\n",
      " |  \n",
      " |  .. note:: :class:`SparkContext` instance is not supported to share across multiple\n",
      " |      processes out of the box, and PySpark does not guarantee multi-processing execution.\n",
      " |      Use threads instead for concurrent processing purpose.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |      Enable 'with SparkContext(...) as sc: app(sc)' syntax.\n",
      " |  \n",
      " |  __exit__(self, type, value, trace)\n",
      " |      Enable 'with SparkContext(...) as sc: app' syntax.\n",
      " |      \n",
      " |      Specifically stop the context on exit of the with block.\n",
      " |  \n",
      " |  __getnewargs__(self)\n",
      " |  \n",
      " |  __init__(self, master=None, appName=None, sparkHome=None, pyFiles=None, environment=None, batchSize=0, serializer=PickleSerializer(), conf=None, gateway=None, jsc=None, profiler_cls=<class 'pyspark.profiler.BasicProfiler'>)\n",
      " |      Create a new SparkContext. At least the master and app name should be set,\n",
      " |      either through the named parameters here or through `conf`.\n",
      " |      \n",
      " |      :param master: Cluster URL to connect to\n",
      " |             (e.g. mesos://host:port, spark://host:port, local[4]).\n",
      " |      :param appName: A name for your job, to display on the cluster web UI.\n",
      " |      :param sparkHome: Location where Spark is installed on cluster nodes.\n",
      " |      :param pyFiles: Collection of .zip or .py files to send to the cluster\n",
      " |             and add to PYTHONPATH.  These can be paths on the local file\n",
      " |             system or HDFS, HTTP, HTTPS, or FTP URLs.\n",
      " |      :param environment: A dictionary of environment variables to set on\n",
      " |             worker nodes.\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. Set 1 to disable batching, 0 to automatically choose\n",
      " |             the batch size based on object sizes, or -1 to use an unlimited\n",
      " |             batch size\n",
      " |      :param serializer: The serializer for RDDs.\n",
      " |      :param conf: A :class:`SparkConf` object setting Spark properties.\n",
      " |      :param gateway: Use an existing gateway and JVM, otherwise a new JVM\n",
      " |             will be instantiated.\n",
      " |      :param jsc: The JavaSparkContext instance (optional).\n",
      " |      :param profiler_cls: A class of custom Profiler used to do profiling\n",
      " |             (default is pyspark.profiler.BasicProfiler).\n",
      " |      \n",
      " |      \n",
      " |      >>> from pyspark.context import SparkContext\n",
      " |      >>> sc = SparkContext('local', 'test')\n",
      " |      \n",
      " |      >>> sc2 = SparkContext('local', 'test2') # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      ValueError:...\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  accumulator(self, value, accum_param=None)\n",
      " |      Create an :class:`Accumulator` with the given initial value, using a given\n",
      " |      :class:`AccumulatorParam` helper object to define how to add values of the\n",
      " |      data type if provided. Default AccumulatorParams are used for integers\n",
      " |      and floating-point numbers if you do not provide one. For other types,\n",
      " |      a custom AccumulatorParam can be used.\n",
      " |  \n",
      " |  addFile(self, path, recursive=False)\n",
      " |      Add a file to be downloaded with this Spark job on every node.\n",
      " |      The `path` passed can be either a local file, a file in HDFS\n",
      " |      (or other Hadoop-supported filesystems), or an HTTP, HTTPS or\n",
      " |      FTP URI.\n",
      " |      \n",
      " |      To access the file in Spark jobs, use :meth:`SparkFiles.get` with the\n",
      " |      filename to find its download location.\n",
      " |      \n",
      " |      A directory can be given if the recursive option is set to True.\n",
      " |      Currently directories are only supported for Hadoop-supported filesystems.\n",
      " |      \n",
      " |      .. note:: A path can be added only once. Subsequent additions of the same path are ignored.\n",
      " |      \n",
      " |      >>> from pyspark import SparkFiles\n",
      " |      >>> path = os.path.join(tempdir, \"test.txt\")\n",
      " |      >>> with open(path, \"w\") as testFile:\n",
      " |      ...    _ = testFile.write(\"100\")\n",
      " |      >>> sc.addFile(path)\n",
      " |      >>> def func(iterator):\n",
      " |      ...    with open(SparkFiles.get(\"test.txt\")) as testFile:\n",
      " |      ...        fileVal = int(testFile.readline())\n",
      " |      ...        return [x * fileVal for x in iterator]\n",
      " |      >>> sc.parallelize([1, 2, 3, 4]).mapPartitions(func).collect()\n",
      " |      [100, 200, 300, 400]\n",
      " |  \n",
      " |  addPyFile(self, path)\n",
      " |      Add a .py or .zip dependency for all tasks to be executed on this\n",
      " |      SparkContext in the future.  The `path` passed can be either a local\n",
      " |      file, a file in HDFS (or other Hadoop-supported filesystems), or an\n",
      " |      HTTP, HTTPS or FTP URI.\n",
      " |      \n",
      " |      .. note:: A path can be added only once. Subsequent additions of the same path are ignored.\n",
      " |  \n",
      " |  binaryFiles(self, path, minPartitions=None)\n",
      " |      Read a directory of binary files from HDFS, a local file system\n",
      " |      (available on all nodes), or any Hadoop-supported file system URI\n",
      " |      as a byte array. Each file is read as a single record and returned\n",
      " |      in a key-value pair, where the key is the path of each file, the\n",
      " |      value is the content of each file.\n",
      " |      \n",
      " |      .. note:: Small files are preferred, large file is also allowable, but\n",
      " |          may cause bad performance.\n",
      " |  \n",
      " |  binaryRecords(self, path, recordLength)\n",
      " |      Load data from a flat binary file, assuming each record is a set of numbers\n",
      " |      with the specified numerical format (see ByteBuffer), and the number of\n",
      " |      bytes per record is constant.\n",
      " |      \n",
      " |      :param path: Directory to the input data files\n",
      " |      :param recordLength: The length at which to split the records\n",
      " |  \n",
      " |  broadcast(self, value)\n",
      " |      Broadcast a read-only variable to the cluster, returning a :class:`Broadcast`\n",
      " |      object for reading it in distributed functions. The variable will\n",
      " |      be sent to each cluster only once.\n",
      " |  \n",
      " |  cancelAllJobs(self)\n",
      " |      Cancel all jobs that have been scheduled or are running.\n",
      " |  \n",
      " |  cancelJobGroup(self, groupId)\n",
      " |      Cancel active jobs for the specified group. See :meth:`SparkContext.setJobGroup`.\n",
      " |      for more information.\n",
      " |  \n",
      " |  dump_profiles(self, path)\n",
      " |      Dump the profile stats into directory `path`\n",
      " |  \n",
      " |  emptyRDD(self)\n",
      " |      Create an RDD that has no partitions or elements.\n",
      " |  \n",
      " |  getConf(self)\n",
      " |  \n",
      " |  getLocalProperty(self, key)\n",
      " |      Get a local property set in this thread, or null if it is missing. See\n",
      " |      :meth:`setLocalProperty`.\n",
      " |  \n",
      " |  hadoopFile(self, path, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n",
      " |      Read an 'old' Hadoop InputFormat with arbitrary key and value class from HDFS,\n",
      " |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n",
      " |      The mechanism is the same as for sc.sequenceFile.\n",
      " |      \n",
      " |      A Hadoop configuration can be passed in as a Python dict. This will be converted into a\n",
      " |      Configuration in Java.\n",
      " |      \n",
      " |      :param path: path to Hadoop file\n",
      " |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapred.TextInputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: Hadoop configuration, passed in as a dict\n",
      " |             (None by default)\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. (default 0, choose batchSize automatically)\n",
      " |  \n",
      " |  hadoopRDD(self, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n",
      " |      Read an 'old' Hadoop InputFormat with arbitrary key and value class, from an arbitrary\n",
      " |      Hadoop configuration, which is passed in as a Python dict.\n",
      " |      This will be converted into a Configuration in Java.\n",
      " |      The mechanism is the same as for sc.sequenceFile.\n",
      " |      \n",
      " |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapred.TextInputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: Hadoop configuration, passed in as a dict\n",
      " |             (None by default)\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. (default 0, choose batchSize automatically)\n",
      " |  \n",
      " |  newAPIHadoopFile(self, path, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n",
      " |      Read a 'new API' Hadoop InputFormat with arbitrary key and value class from HDFS,\n",
      " |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n",
      " |      The mechanism is the same as for sc.sequenceFile.\n",
      " |      \n",
      " |      A Hadoop configuration can be passed in as a Python dict. This will be converted into a\n",
      " |      Configuration in Java\n",
      " |      \n",
      " |      :param path: path to Hadoop file\n",
      " |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: Hadoop configuration, passed in as a dict\n",
      " |             (None by default)\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. (default 0, choose batchSize automatically)\n",
      " |  \n",
      " |  newAPIHadoopRDD(self, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n",
      " |      Read a 'new API' Hadoop InputFormat with arbitrary key and value class, from an arbitrary\n",
      " |      Hadoop configuration, which is passed in as a Python dict.\n",
      " |      This will be converted into a Configuration in Java.\n",
      " |      The mechanism is the same as for sc.sequenceFile.\n",
      " |      \n",
      " |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: Hadoop configuration, passed in as a dict\n",
      " |             (None by default)\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. (default 0, choose batchSize automatically)\n",
      " |  \n",
      " |  parallelize(self, c, numSlices=None)\n",
      " |      Distribute a local Python collection to form an RDD. Using xrange\n",
      " |      is recommended if the input represents a range for performance.\n",
      " |      \n",
      " |      >>> sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()\n",
      " |      [[0], [2], [3], [4], [6]]\n",
      " |      >>> sc.parallelize(xrange(0, 6, 2), 5).glom().collect()\n",
      " |      [[], [0], [], [2], [4]]\n",
      " |  \n",
      " |  pickleFile(self, name, minPartitions=None)\n",
      " |      Load an RDD previously saved using :meth:`RDD.saveAsPickleFile` method.\n",
      " |      \n",
      " |      >>> tmpFile = NamedTemporaryFile(delete=True)\n",
      " |      >>> tmpFile.close()\n",
      " |      >>> sc.parallelize(range(10)).saveAsPickleFile(tmpFile.name, 5)\n",
      " |      >>> sorted(sc.pickleFile(tmpFile.name, 3).collect())\n",
      " |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      " |  \n",
      " |  range(self, start, end=None, step=1, numSlices=None)\n",
      " |      Create a new RDD of int containing elements from `start` to `end`\n",
      " |      (exclusive), increased by `step` every element. Can be called the same\n",
      " |      way as python's built-in range() function. If called with a single argument,\n",
      " |      the argument is interpreted as `end`, and `start` is set to 0.\n",
      " |      \n",
      " |      :param start: the start value\n",
      " |      :param end: the end value (exclusive)\n",
      " |      :param step: the incremental step (default: 1)\n",
      " |      :param numSlices: the number of partitions of the new RDD\n",
      " |      :return: An RDD of int\n",
      " |      \n",
      " |      >>> sc.range(5).collect()\n",
      " |      [0, 1, 2, 3, 4]\n",
      " |      >>> sc.range(2, 4).collect()\n",
      " |      [2, 3]\n",
      " |      >>> sc.range(1, 7, 2).collect()\n",
      " |      [1, 3, 5]\n",
      " |  \n",
      " |  runJob(self, rdd, partitionFunc, partitions=None, allowLocal=False)\n",
      " |      Executes the given partitionFunc on the specified set of partitions,\n",
      " |      returning the result as an array of elements.\n",
      " |      \n",
      " |      If 'partitions' is not specified, this will run over all partitions.\n",
      " |      \n",
      " |      >>> myRDD = sc.parallelize(range(6), 3)\n",
      " |      >>> sc.runJob(myRDD, lambda part: [x * x for x in part])\n",
      " |      [0, 1, 4, 9, 16, 25]\n",
      " |      \n",
      " |      >>> myRDD = sc.parallelize(range(6), 3)\n",
      " |      >>> sc.runJob(myRDD, lambda part: [x * x for x in part], [0, 2], True)\n",
      " |      [0, 1, 16, 25]\n",
      " |  \n",
      " |  sequenceFile(self, path, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, minSplits=None, batchSize=0)\n",
      " |      Read a Hadoop SequenceFile with arbitrary key and value Writable class from HDFS,\n",
      " |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n",
      " |      The mechanism is as follows:\n",
      " |      \n",
      " |          1. A Java RDD is created from the SequenceFile or other InputFormat, and the key\n",
      " |             and value Writable classes\n",
      " |          2. Serialization is attempted via Pyrolite pickling\n",
      " |          3. If this fails, the fallback is to call 'toString' on each key and value\n",
      " |          4. :class:`PickleSerializer` is used to deserialize pickled objects on the Python side\n",
      " |      \n",
      " |      :param path: path to sequncefile\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      :param keyConverter:\n",
      " |      :param valueConverter:\n",
      " |      :param minSplits: minimum splits in dataset\n",
      " |             (default min(2, sc.defaultParallelism))\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. (default 0, choose batchSize automatically)\n",
      " |  \n",
      " |  setCheckpointDir(self, dirName)\n",
      " |      Set the directory under which RDDs are going to be checkpointed. The\n",
      " |      directory must be an HDFS path if running on a cluster.\n",
      " |  \n",
      " |  setJobDescription(self, value)\n",
      " |      Set a human readable description of the current job.\n",
      " |      \n",
      " |      .. note:: Currently, setting a job description (set to local properties) with multiple\n",
      " |          threads does not properly work. Internally threads on PVM and JVM are not synced,\n",
      " |          and JVM thread can be reused for multiple threads on PVM, which fails to isolate\n",
      " |          local properties for each thread on PVM.\n",
      " |      \n",
      " |          To work around this, you can set `PYSPARK_PIN_THREAD` to\n",
      " |          `'true'` (see SPARK-22340). However, note that it cannot inherit the local properties\n",
      " |          from the parent thread although it isolates each thread on PVM and JVM with its own\n",
      " |          local properties.\n",
      " |      \n",
      " |          To work around this, you should manually copy and set the local\n",
      " |          properties from the parent thread to the child thread when you create another thread.\n",
      " |  \n",
      " |  setJobGroup(self, groupId, description, interruptOnCancel=False)\n",
      " |      Assigns a group ID to all the jobs started by this thread until the group ID is set to a\n",
      " |      different value or cleared.\n",
      " |      \n",
      " |      Often, a unit of execution in an application consists of multiple Spark actions or jobs.\n",
      " |      Application programmers can use this method to group all those jobs together and give a\n",
      " |      group description. Once set, the Spark web UI will associate such jobs with this group.\n",
      " |      \n",
      " |      The application can use :meth:`SparkContext.cancelJobGroup` to cancel all\n",
      " |      running jobs in this group.\n",
      " |      \n",
      " |      >>> import threading\n",
      " |      >>> from time import sleep\n",
      " |      >>> result = \"Not Set\"\n",
      " |      >>> lock = threading.Lock()\n",
      " |      >>> def map_func(x):\n",
      " |      ...     sleep(100)\n",
      " |      ...     raise Exception(\"Task should have been cancelled\")\n",
      " |      >>> def start_job(x):\n",
      " |      ...     global result\n",
      " |      ...     try:\n",
      " |      ...         sc.setJobGroup(\"job_to_cancel\", \"some description\")\n",
      " |      ...         result = sc.parallelize(range(x)).map(map_func).collect()\n",
      " |      ...     except Exception as e:\n",
      " |      ...         result = \"Cancelled\"\n",
      " |      ...     lock.release()\n",
      " |      >>> def stop_job():\n",
      " |      ...     sleep(5)\n",
      " |      ...     sc.cancelJobGroup(\"job_to_cancel\")\n",
      " |      >>> suppress = lock.acquire()\n",
      " |      >>> suppress = threading.Thread(target=start_job, args=(10,)).start()\n",
      " |      >>> suppress = threading.Thread(target=stop_job).start()\n",
      " |      >>> suppress = lock.acquire()\n",
      " |      >>> print(result)\n",
      " |      Cancelled\n",
      " |      \n",
      " |      If interruptOnCancel is set to true for the job group, then job cancellation will result\n",
      " |      in Thread.interrupt() being called on the job's executor threads. This is useful to help\n",
      " |      ensure that the tasks are actually stopped in a timely manner, but is off by default due\n",
      " |      to HDFS-1208, where HDFS may respond to Thread.interrupt() by marking nodes as dead.\n",
      " |      \n",
      " |      .. note:: Currently, setting a group ID (set to local properties) with multiple threads\n",
      " |          does not properly work. Internally threads on PVM and JVM are not synced, and JVM\n",
      " |          thread can be reused for multiple threads on PVM, which fails to isolate local\n",
      " |          properties for each thread on PVM.\n",
      " |      \n",
      " |          To work around this, you can set `PYSPARK_PIN_THREAD` to\n",
      " |          `'true'` (see SPARK-22340). However, note that it cannot inherit the local properties\n",
      " |          from the parent thread although it isolates each thread on PVM and JVM with its own\n",
      " |          local properties.\n",
      " |      \n",
      " |          To work around this, you should manually copy and set the local\n",
      " |          properties from the parent thread to the child thread when you create another thread.\n",
      " |  \n",
      " |  setLocalProperty(self, key, value)\n",
      " |      Set a local property that affects jobs submitted from this thread, such as the\n",
      " |      Spark fair scheduler pool.\n",
      " |      \n",
      " |      .. note:: Currently, setting a local property with multiple threads does not properly work.\n",
      " |          Internally threads on PVM and JVM are not synced, and JVM thread\n",
      " |          can be reused for multiple threads on PVM, which fails to isolate local properties\n",
      " |          for each thread on PVM.\n",
      " |      \n",
      " |          To work around this, you can set `PYSPARK_PIN_THREAD` to\n",
      " |          `'true'` (see SPARK-22340). However, note that it cannot inherit the local properties\n",
      " |          from the parent thread although it isolates each thread on PVM and JVM with its own\n",
      " |          local properties.\n",
      " |      \n",
      " |          To work around this, you should manually copy and set the local\n",
      " |          properties from the parent thread to the child thread when you create another thread.\n",
      " |  \n",
      " |  setLogLevel(self, logLevel)\n",
      " |      Control our logLevel. This overrides any user-defined log settings.\n",
      " |      Valid log levels include: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN\n",
      " |  \n",
      " |  show_profiles(self)\n",
      " |      Print the profile stats to stdout\n",
      " |  \n",
      " |  sparkUser(self)\n",
      " |      Get SPARK_USER for user who is running SparkContext.\n",
      " |  \n",
      " |  statusTracker(self)\n",
      " |      Return :class:`StatusTracker` object\n",
      " |  \n",
      " |  stop(self)\n",
      " |      Shut down the SparkContext.\n",
      " |  \n",
      " |  textFile(self, name, minPartitions=None, use_unicode=True)\n",
      " |      Read a text file from HDFS, a local file system (available on all\n",
      " |      nodes), or any Hadoop-supported file system URI, and return it as an\n",
      " |      RDD of Strings.\n",
      " |      The text files must be encoded as UTF-8.\n",
      " |      \n",
      " |      If use_unicode is False, the strings will be kept as `str` (encoding\n",
      " |      as `utf-8`), which is faster and smaller than unicode. (Added in\n",
      " |      Spark 1.2)\n",
      " |      \n",
      " |      >>> path = os.path.join(tempdir, \"sample-text.txt\")\n",
      " |      >>> with open(path, \"w\") as testFile:\n",
      " |      ...    _ = testFile.write(\"Hello world!\")\n",
      " |      >>> textFile = sc.textFile(path)\n",
      " |      >>> textFile.collect()\n",
      " |      ['Hello world!']\n",
      " |  \n",
      " |  union(self, rdds)\n",
      " |      Build the union of a list of RDDs.\n",
      " |      \n",
      " |      This supports unions() of RDDs with different serialized formats,\n",
      " |      although this forces them to be reserialized using the default\n",
      " |      serializer:\n",
      " |      \n",
      " |      >>> path = os.path.join(tempdir, \"union-text.txt\")\n",
      " |      >>> with open(path, \"w\") as testFile:\n",
      " |      ...    _ = testFile.write(\"Hello\")\n",
      " |      >>> textFile = sc.textFile(path)\n",
      " |      >>> textFile.collect()\n",
      " |      ['Hello']\n",
      " |      >>> parallelized = sc.parallelize([\"World!\"])\n",
      " |      >>> sorted(sc.union([textFile, parallelized]).collect())\n",
      " |      ['Hello', 'World!']\n",
      " |  \n",
      " |  wholeTextFiles(self, path, minPartitions=None, use_unicode=True)\n",
      " |      Read a directory of text files from HDFS, a local file system\n",
      " |      (available on all nodes), or any  Hadoop-supported file system\n",
      " |      URI. Each file is read as a single record and returned in a\n",
      " |      key-value pair, where the key is the path of each file, the\n",
      " |      value is the content of each file.\n",
      " |      The text files must be encoded as UTF-8.\n",
      " |      \n",
      " |      If use_unicode is False, the strings will be kept as `str` (encoding\n",
      " |      as `utf-8`), which is faster and smaller than unicode. (Added in\n",
      " |      Spark 1.2)\n",
      " |      \n",
      " |      For example, if you have the following files:\n",
      " |      \n",
      " |      .. code-block:: text\n",
      " |      \n",
      " |          hdfs://a-hdfs-path/part-00000\n",
      " |          hdfs://a-hdfs-path/part-00001\n",
      " |          ...\n",
      " |          hdfs://a-hdfs-path/part-nnnnn\n",
      " |      \n",
      " |      Do ``rdd = sparkContext.wholeTextFiles(\"hdfs://a-hdfs-path\")``,\n",
      " |      then ``rdd`` contains:\n",
      " |      \n",
      " |      .. code-block:: text\n",
      " |      \n",
      " |          (a-hdfs-path/part-00000, its content)\n",
      " |          (a-hdfs-path/part-00001, its content)\n",
      " |          ...\n",
      " |          (a-hdfs-path/part-nnnnn, its content)\n",
      " |      \n",
      " |      .. note:: Small files are preferred, as each file will be loaded\n",
      " |          fully in memory.\n",
      " |      \n",
      " |      >>> dirPath = os.path.join(tempdir, \"files\")\n",
      " |      >>> os.mkdir(dirPath)\n",
      " |      >>> with open(os.path.join(dirPath, \"1.txt\"), \"w\") as file1:\n",
      " |      ...    _ = file1.write(\"1\")\n",
      " |      >>> with open(os.path.join(dirPath, \"2.txt\"), \"w\") as file2:\n",
      " |      ...    _ = file2.write(\"2\")\n",
      " |      >>> textFiles = sc.wholeTextFiles(dirPath)\n",
      " |      >>> sorted(textFiles.collect())\n",
      " |      [('.../1.txt', '1'), ('.../2.txt', '2')]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  getOrCreate(conf=None) from builtins.type\n",
      " |      Get or instantiate a SparkContext and register it as a singleton object.\n",
      " |      \n",
      " |      :param conf: SparkConf (optional)\n",
      " |  \n",
      " |  setSystemProperty(key, value) from builtins.type\n",
      " |      Set a Java system property, such as spark.executor.memory. This must\n",
      " |      must be invoked before instantiating SparkContext.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  applicationId\n",
      " |      A unique identifier for the Spark application.\n",
      " |      Its format depends on the scheduler implementation.\n",
      " |      \n",
      " |      * in case of local spark app something like 'local-1433865536131'\n",
      " |      * in case of YARN something like 'application_1433865536131_34483'\n",
      " |      \n",
      " |      >>> sc.applicationId  # doctest: +ELLIPSIS\n",
      " |      'local-...'\n",
      " |  \n",
      " |  defaultMinPartitions\n",
      " |      Default min number of partitions for Hadoop RDDs when not given by user\n",
      " |  \n",
      " |  defaultParallelism\n",
      " |      Default level of parallelism to use when not given by user (e.g. for\n",
      " |      reduce tasks)\n",
      " |  \n",
      " |  resources\n",
      " |  \n",
      " |  startTime\n",
      " |      Return the epoch time when the Spark Context was started.\n",
      " |  \n",
      " |  uiWebUrl\n",
      " |      Return the URL of the SparkUI instance started by this SparkContext\n",
      " |  \n",
      " |  version\n",
      " |      The version of Spark on which this application is running.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  PACKAGE_EXTENSIONS = ('.zip', '.egg', '.jar')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PACKAGE_EXTENSIONS',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getnewargs__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_accumulatorServer',\n",
       " '_active_spark_context',\n",
       " '_batchSize',\n",
       " '_callsite',\n",
       " '_checkpointFile',\n",
       " '_conf',\n",
       " '_dictToJavaMap',\n",
       " '_do_init',\n",
       " '_encryption_enabled',\n",
       " '_ensure_initialized',\n",
       " '_gateway',\n",
       " '_getJavaStorageLevel',\n",
       " '_initialize_context',\n",
       " '_javaAccumulator',\n",
       " '_jsc',\n",
       " '_jvm',\n",
       " '_lock',\n",
       " '_next_accum_id',\n",
       " '_pickled_broadcast_vars',\n",
       " '_python_includes',\n",
       " '_repr_html_',\n",
       " '_serialize_to_jvm',\n",
       " '_temp_dir',\n",
       " '_unbatched_serializer',\n",
       " 'accumulator',\n",
       " 'addFile',\n",
       " 'addPyFile',\n",
       " 'appName',\n",
       " 'applicationId',\n",
       " 'binaryFiles',\n",
       " 'binaryRecords',\n",
       " 'broadcast',\n",
       " 'cancelAllJobs',\n",
       " 'cancelJobGroup',\n",
       " 'defaultMinPartitions',\n",
       " 'defaultParallelism',\n",
       " 'dump_profiles',\n",
       " 'emptyRDD',\n",
       " 'environment',\n",
       " 'getConf',\n",
       " 'getLocalProperty',\n",
       " 'getOrCreate',\n",
       " 'hadoopFile',\n",
       " 'hadoopRDD',\n",
       " 'master',\n",
       " 'newAPIHadoopFile',\n",
       " 'newAPIHadoopRDD',\n",
       " 'parallelize',\n",
       " 'pickleFile',\n",
       " 'profiler_collector',\n",
       " 'pythonExec',\n",
       " 'pythonVer',\n",
       " 'range',\n",
       " 'resources',\n",
       " 'runJob',\n",
       " 'sequenceFile',\n",
       " 'serializer',\n",
       " 'setCheckpointDir',\n",
       " 'setJobDescription',\n",
       " 'setJobGroup',\n",
       " 'setLocalProperty',\n",
       " 'setLogLevel',\n",
       " 'setSystemProperty',\n",
       " 'show_profiles',\n",
       " 'sparkHome',\n",
       " 'sparkUser',\n",
       " 'startTime',\n",
       " 'statusTracker',\n",
       " 'stop',\n",
       " 'textFile',\n",
       " 'uiWebUrl',\n",
       " 'union',\n",
       " 'version',\n",
       " 'wholeTextFiles']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into Spark\n",
    "## RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4]\n",
    "distributedData = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date,Open,High,Low,Close,AdjClose,Volume',\n",
       " '2019-07-15,248.000000,254.419998,244.860001,253.500000,253.500000,11000100',\n",
       " '2019-07-16,249.300003,253.529999,247.929993,252.380005,252.380005,8149000',\n",
       " '2019-07-17,255.669998,258.309998,253.350006,254.860001,254.860001,9764700',\n",
       " '2019-07-18,255.050003,255.750000,251.889999,253.539993,253.539993,4764500']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tesla_file = 'TSLA.csv'\n",
    "tesla_rdd = sc.textFile(tesla_file)\n",
    "tesla_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tesla_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_rdd = tesla_rdd.map(lambda row: row.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Date', 'Open', 'High', 'Low', 'Close', 'AdjClose', 'Volume'],\n",
       " ['2019-07-15',\n",
       "  '248.000000',\n",
       "  '254.419998',\n",
       "  '244.860001',\n",
       "  '253.500000',\n",
       "  '253.500000',\n",
       "  '11000100'],\n",
       " ['2019-07-16',\n",
       "  '249.300003',\n",
       "  '253.529999',\n",
       "  '247.929993',\n",
       "  '252.380005',\n",
       "  '252.380005',\n",
       "  '8149000'],\n",
       " ['2019-07-17',\n",
       "  '255.669998',\n",
       "  '258.309998',\n",
       "  '253.350006',\n",
       "  '254.860001',\n",
       "  '254.860001',\n",
       "  '9764700'],\n",
       " ['2019-07-18',\n",
       "  '255.050003',\n",
       "  '255.750000',\n",
       "  '251.889999',\n",
       "  '253.539993',\n",
       "  '253.539993',\n",
       "  '4764500'],\n",
       " ['2019-07-19',\n",
       "  '255.690002',\n",
       "  '259.959991',\n",
       "  '254.619995',\n",
       "  '258.179993',\n",
       "  '258.179993',\n",
       "  '7048400'],\n",
       " ['2019-07-22',\n",
       "  '258.750000',\n",
       "  '262.149994',\n",
       "  '254.190002',\n",
       "  '255.679993',\n",
       "  '255.679993',\n",
       "  '6842400'],\n",
       " ['2019-07-23',\n",
       "  '256.709991',\n",
       "  '260.480011',\n",
       "  '254.500000',\n",
       "  '260.170013',\n",
       "  '260.170013',\n",
       "  '5023100'],\n",
       " ['2019-07-24',\n",
       "  '259.170013',\n",
       "  '266.070007',\n",
       "  '258.160004',\n",
       "  '264.880005',\n",
       "  '264.880005',\n",
       "  '11072800'],\n",
       " ['2019-07-25',\n",
       "  '233.500000',\n",
       "  '234.500000',\n",
       "  '225.550003',\n",
       "  '228.820007',\n",
       "  '228.820007',\n",
       "  '22418300'],\n",
       " ['2019-07-26',\n",
       "  '226.919998',\n",
       "  '230.259995',\n",
       "  '222.250000',\n",
       "  '228.039993',\n",
       "  '228.039993',\n",
       "  '10027700'],\n",
       " ['2019-07-29',\n",
       "  '227.089996',\n",
       "  '235.940002',\n",
       "  '226.029999',\n",
       "  '235.770004',\n",
       "  '235.770004',\n",
       "  '9273300'],\n",
       " ['2019-07-30',\n",
       "  '232.899994',\n",
       "  '243.360001',\n",
       "  '232.179993',\n",
       "  '242.259995',\n",
       "  '242.259995',\n",
       "  '8109000'],\n",
       " ['2019-07-31',\n",
       "  '243.000000',\n",
       "  '246.679993',\n",
       "  '236.649994',\n",
       "  '241.610001',\n",
       "  '241.610001',\n",
       "  '9178200'],\n",
       " ['2019-08-01',\n",
       "  '242.649994',\n",
       "  '244.509995',\n",
       "  '231.770004',\n",
       "  '233.850006',\n",
       "  '233.850006',\n",
       "  '8259500'],\n",
       " ['2019-08-02',\n",
       "  '231.350006',\n",
       "  '236.270004',\n",
       "  '229.229996',\n",
       "  '234.339996',\n",
       "  '234.339996',\n",
       "  '6136500'],\n",
       " ['2019-08-05',\n",
       "  '229.600006',\n",
       "  '231.369995',\n",
       "  '225.779999',\n",
       "  '228.320007',\n",
       "  '228.320007',\n",
       "  '7028300'],\n",
       " ['2019-08-06',\n",
       "  '231.880005',\n",
       "  '232.500000',\n",
       "  '225.750000',\n",
       "  '230.750000',\n",
       "  '230.750000',\n",
       "  '5564200'],\n",
       " ['2019-08-07',\n",
       "  '226.500000',\n",
       "  '233.570007',\n",
       "  '225.800003',\n",
       "  '233.419998',\n",
       "  '233.419998',\n",
       "  '4776500'],\n",
       " ['2019-08-08',\n",
       "  '234.449997',\n",
       "  '239.800003',\n",
       "  '232.649994',\n",
       "  '238.300003',\n",
       "  '238.300003',\n",
       "  '5274300'],\n",
       " ['2019-08-09',\n",
       "  '236.050003',\n",
       "  '238.960007',\n",
       "  '233.809998',\n",
       "  '235.009995',\n",
       "  '235.009995',\n",
       "  '3898200'],\n",
       " ['2019-08-12',\n",
       "  '232.990005',\n",
       "  '235.770004',\n",
       "  '228.750000',\n",
       "  '229.009995',\n",
       "  '229.009995',\n",
       "  '4663900'],\n",
       " ['2019-08-13',\n",
       "  '228.809998',\n",
       "  '236.000000',\n",
       "  '227.550003',\n",
       "  '235.000000',\n",
       "  '235.000000',\n",
       "  '4848100'],\n",
       " ['2019-08-14',\n",
       "  '231.210007',\n",
       "  '231.500000',\n",
       "  '216.690002',\n",
       "  '219.619995',\n",
       "  '219.619995',\n",
       "  '9562600'],\n",
       " ['2019-08-15',\n",
       "  '220.860001',\n",
       "  '221.559998',\n",
       "  '211.550003',\n",
       "  '215.639999',\n",
       "  '215.639999',\n",
       "  '8159600'],\n",
       " ['2019-08-16',\n",
       "  '216.660004',\n",
       "  '222.240005',\n",
       "  '216.020004',\n",
       "  '219.940002',\n",
       "  '219.940002',\n",
       "  '5098500'],\n",
       " ['2019-08-19',\n",
       "  '224.210007',\n",
       "  '227.830002',\n",
       "  '221.699997',\n",
       "  '226.830002',\n",
       "  '226.830002',\n",
       "  '5309600'],\n",
       " ['2019-08-20',\n",
       "  '227.619995',\n",
       "  '229.089996',\n",
       "  '224.539993',\n",
       "  '225.860001',\n",
       "  '225.860001',\n",
       "  '4125200'],\n",
       " ['2019-08-21',\n",
       "  '222.009995',\n",
       "  '223.220001',\n",
       "  '217.600006',\n",
       "  '220.830002',\n",
       "  '220.830002',\n",
       "  '7794300'],\n",
       " ['2019-08-22',\n",
       "  '222.800003',\n",
       "  '225.399994',\n",
       "  '218.220001',\n",
       "  '222.149994',\n",
       "  '222.149994',\n",
       "  '6559000'],\n",
       " ['2019-08-23',\n",
       "  '219.970001',\n",
       "  '221.169998',\n",
       "  '211.000000',\n",
       "  '211.399994',\n",
       "  '211.399994',\n",
       "  '8538600'],\n",
       " ['2019-08-26',\n",
       "  '213.600006',\n",
       "  '215.020004',\n",
       "  '211.539993',\n",
       "  '215.000000',\n",
       "  '215.000000',\n",
       "  '5051900'],\n",
       " ['2019-08-27',\n",
       "  '215.740005',\n",
       "  '218.800003',\n",
       "  '212.029999',\n",
       "  '214.080002',\n",
       "  '214.080002',\n",
       "  '5416200'],\n",
       " ['2019-08-28',\n",
       "  '213.690002',\n",
       "  '217.250000',\n",
       "  '212.309998',\n",
       "  '215.589996',\n",
       "  '215.589996',\n",
       "  '3225500'],\n",
       " ['2019-08-29',\n",
       "  '219.000000',\n",
       "  '223.399994',\n",
       "  '218.000000',\n",
       "  '221.710007',\n",
       "  '221.710007',\n",
       "  '5179500'],\n",
       " ['2019-08-30',\n",
       "  '229.149994',\n",
       "  '232.440002',\n",
       "  '224.210007',\n",
       "  '225.610001',\n",
       "  '225.610001',\n",
       "  '9320600'],\n",
       " ['2019-09-03',\n",
       "  '224.080002',\n",
       "  '228.949997',\n",
       "  '223.160004',\n",
       "  '225.009995',\n",
       "  '225.009995',\n",
       "  '5354100'],\n",
       " ['2019-09-04',\n",
       "  '226.889999',\n",
       "  '228.460007',\n",
       "  '219.210007',\n",
       "  '220.679993',\n",
       "  '220.679993',\n",
       "  '5761000'],\n",
       " ['2019-09-05',\n",
       "  '222.500000',\n",
       "  '229.800003',\n",
       "  '220.850006',\n",
       "  '229.580002',\n",
       "  '229.580002',\n",
       "  '7395300'],\n",
       " ['2019-09-06',\n",
       "  '227.199997',\n",
       "  '229.639999',\n",
       "  '225.169998',\n",
       "  '227.449997',\n",
       "  '227.449997',\n",
       "  '4189400'],\n",
       " ['2019-09-09',\n",
       "  '230.000000',\n",
       "  '233.759995',\n",
       "  '229.229996',\n",
       "  '231.789993',\n",
       "  '231.789993',\n",
       "  '4802700'],\n",
       " ['2019-09-10',\n",
       "  '230.800003',\n",
       "  '235.539993',\n",
       "  '228.940002',\n",
       "  '235.539993',\n",
       "  '235.539993',\n",
       "  '4883700'],\n",
       " ['2019-09-11',\n",
       "  '237.380005',\n",
       "  '248.169998',\n",
       "  '236.000000',\n",
       "  '247.100006',\n",
       "  '247.100006',\n",
       "  '10042800'],\n",
       " ['2019-09-12',\n",
       "  '247.699997',\n",
       "  '253.500000',\n",
       "  '244.399994',\n",
       "  '245.869995',\n",
       "  '245.869995',\n",
       "  '8581200'],\n",
       " ['2019-09-13',\n",
       "  '246.960007',\n",
       "  '248.449997',\n",
       "  '244.869995',\n",
       "  '245.199997',\n",
       "  '245.199997',\n",
       "  '5313100'],\n",
       " ['2019-09-16',\n",
       "  '246.000000',\n",
       "  '247.429993',\n",
       "  '241.169998',\n",
       "  '242.809998',\n",
       "  '242.809998',\n",
       "  '4728100'],\n",
       " ['2019-09-17',\n",
       "  '242.470001',\n",
       "  '245.600006',\n",
       "  '240.369995',\n",
       "  '244.789993',\n",
       "  '244.789993',\n",
       "  '3865400'],\n",
       " ['2019-09-18',\n",
       "  '245.000000',\n",
       "  '248.169998',\n",
       "  '242.369995',\n",
       "  '243.490005',\n",
       "  '243.490005',\n",
       "  '4170200'],\n",
       " ['2019-09-19',\n",
       "  '246.000000',\n",
       "  '247.940002',\n",
       "  '244.839996',\n",
       "  '246.600006',\n",
       "  '246.600006',\n",
       "  '4795800'],\n",
       " ['2019-09-20',\n",
       "  '246.490005',\n",
       "  '246.949997',\n",
       "  '238.160004',\n",
       "  '240.619995',\n",
       "  '240.619995',\n",
       "  '6353000'],\n",
       " ['2019-09-23',\n",
       "  '240.000000',\n",
       "  '245.179993',\n",
       "  '239.220001',\n",
       "  '241.229996',\n",
       "  '241.229996',\n",
       "  '4340200'],\n",
       " ['2019-09-24',\n",
       "  '241.520004',\n",
       "  '241.990005',\n",
       "  '222.610001',\n",
       "  '223.210007',\n",
       "  '223.210007',\n",
       "  '12891500'],\n",
       " ['2019-09-25',\n",
       "  '224.559998',\n",
       "  '228.979996',\n",
       "  '218.360001',\n",
       "  '228.699997',\n",
       "  '228.699997',\n",
       "  '9427100'],\n",
       " ['2019-09-26',\n",
       "  '230.660004',\n",
       "  '243.309998',\n",
       "  '227.399994',\n",
       "  '242.559998',\n",
       "  '242.559998',\n",
       "  '11884500'],\n",
       " ['2019-09-27',\n",
       "  '242.199997',\n",
       "  '248.710007',\n",
       "  '238.729996',\n",
       "  '242.130005',\n",
       "  '242.130005',\n",
       "  '11116400'],\n",
       " ['2019-09-30',\n",
       "  '243.000000',\n",
       "  '243.979996',\n",
       "  '236.110001',\n",
       "  '240.869995',\n",
       "  '240.869995',\n",
       "  '5879800'],\n",
       " ['2019-10-01',\n",
       "  '241.500000',\n",
       "  '245.949997',\n",
       "  '239.130005',\n",
       "  '244.690002',\n",
       "  '244.690002',\n",
       "  '6162600'],\n",
       " ['2019-10-02',\n",
       "  '243.289993',\n",
       "  '244.649994',\n",
       "  '239.429993',\n",
       "  '243.130005',\n",
       "  '243.130005',\n",
       "  '5631400'],\n",
       " ['2019-10-03',\n",
       "  '231.860001',\n",
       "  '234.479996',\n",
       "  '224.279999',\n",
       "  '233.029999',\n",
       "  '233.029999',\n",
       "  '15084500'],\n",
       " ['2019-10-04',\n",
       "  '231.610001',\n",
       "  '234.779999',\n",
       "  '228.070007',\n",
       "  '231.429993',\n",
       "  '231.429993',\n",
       "  '7995000'],\n",
       " ['2019-10-07',\n",
       "  '229.800003',\n",
       "  '238.559998',\n",
       "  '228.550003',\n",
       "  '237.720001',\n",
       "  '237.720001',\n",
       "  '8064200'],\n",
       " ['2019-10-08',\n",
       "  '235.869995',\n",
       "  '243.940002',\n",
       "  '234.500000',\n",
       "  '240.050003',\n",
       "  '240.050003',\n",
       "  '8678200'],\n",
       " ['2019-10-09',\n",
       "  '241.320007',\n",
       "  '247.300003',\n",
       "  '240.649994',\n",
       "  '244.529999',\n",
       "  '244.529999',\n",
       "  '6894400'],\n",
       " ['2019-10-10',\n",
       "  '245.279999',\n",
       "  '249.279999',\n",
       "  '241.580002',\n",
       "  '244.740005',\n",
       "  '244.740005',\n",
       "  '6283300'],\n",
       " ['2019-10-11',\n",
       "  '247.149994',\n",
       "  '251.080002',\n",
       "  '246.809998',\n",
       "  '247.889999',\n",
       "  '247.889999',\n",
       "  '8475400'],\n",
       " ['2019-10-14',\n",
       "  '247.899994',\n",
       "  '258.549988',\n",
       "  '247.130005',\n",
       "  '256.959991',\n",
       "  '256.959991',\n",
       "  '10205000'],\n",
       " ['2019-10-15',\n",
       "  '257.700012',\n",
       "  '260.000000',\n",
       "  '254.119995',\n",
       "  '257.890015',\n",
       "  '257.890015',\n",
       "  '6432800'],\n",
       " ['2019-10-16',\n",
       "  '257.390015',\n",
       "  '262.100006',\n",
       "  '256.920013',\n",
       "  '259.750000',\n",
       "  '259.750000',\n",
       "  '6684100'],\n",
       " ['2019-10-17',\n",
       "  '262.500000',\n",
       "  '264.779999',\n",
       "  '260.170013',\n",
       "  '261.970001',\n",
       "  '261.970001',\n",
       "  '4769300'],\n",
       " ['2019-10-18',\n",
       "  '260.700012',\n",
       "  '262.799988',\n",
       "  '255.100006',\n",
       "  '256.950012',\n",
       "  '256.950012',\n",
       "  '5749800'],\n",
       " ['2019-10-21',\n",
       "  '258.329987',\n",
       "  '259.500000',\n",
       "  '250.179993',\n",
       "  '253.500000',\n",
       "  '253.500000',\n",
       "  '5020300'],\n",
       " ['2019-10-22',\n",
       "  '254.320007',\n",
       "  '258.329987',\n",
       "  '250.850006',\n",
       "  '255.580002',\n",
       "  '255.580002',\n",
       "  '4600800'],\n",
       " ['2019-10-23',\n",
       "  '254.500000',\n",
       "  '256.140015',\n",
       "  '251.350006',\n",
       "  '254.679993',\n",
       "  '254.679993',\n",
       "  '5261100'],\n",
       " ['2019-10-24',\n",
       "  '298.369995',\n",
       "  '304.929993',\n",
       "  '289.200012',\n",
       "  '299.679993',\n",
       "  '299.679993',\n",
       "  '29720900'],\n",
       " ['2019-10-25',\n",
       "  '297.720001',\n",
       "  '330.000000',\n",
       "  '296.109985',\n",
       "  '328.130005',\n",
       "  '328.130005',\n",
       "  '30006100'],\n",
       " ['2019-10-28',\n",
       "  '327.540009',\n",
       "  '340.839996',\n",
       "  '322.600006',\n",
       "  '327.709991',\n",
       "  '327.709991',\n",
       "  '18870300'],\n",
       " ['2019-10-29',\n",
       "  '319.989990',\n",
       "  '324.299988',\n",
       "  '314.750000',\n",
       "  '316.220001',\n",
       "  '316.220001',\n",
       "  '12684300'],\n",
       " ['2019-10-30',\n",
       "  '313.000000',\n",
       "  '318.790009',\n",
       "  '309.970001',\n",
       "  '315.010010',\n",
       "  '315.010010',\n",
       "  '9641800'],\n",
       " ['2019-10-31',\n",
       "  '313.100006',\n",
       "  '319.000000',\n",
       "  '313.000000',\n",
       "  '314.920013',\n",
       "  '314.920013',\n",
       "  '5067000'],\n",
       " ['2019-11-01',\n",
       "  '316.320007',\n",
       "  '316.480011',\n",
       "  '309.799988',\n",
       "  '313.309998',\n",
       "  '313.309998',\n",
       "  '6383900'],\n",
       " ['2019-11-04',\n",
       "  '314.799988',\n",
       "  '321.940002',\n",
       "  '309.260010',\n",
       "  '317.470001',\n",
       "  '317.470001',\n",
       "  '8787000'],\n",
       " ['2019-11-05',\n",
       "  '319.619995',\n",
       "  '323.510010',\n",
       "  '316.119995',\n",
       "  '317.220001',\n",
       "  '317.220001',\n",
       "  '6943400'],\n",
       " ['2019-11-06',\n",
       "  '318.000000',\n",
       "  '326.720001',\n",
       "  '314.500000',\n",
       "  '326.579987',\n",
       "  '326.579987',\n",
       "  '7940900'],\n",
       " ['2019-11-07',\n",
       "  '329.140015',\n",
       "  '341.500000',\n",
       "  '328.019989',\n",
       "  '335.540009',\n",
       "  '335.540009',\n",
       "  '14467300'],\n",
       " ['2019-11-08',\n",
       "  '334.500000',\n",
       "  '337.459991',\n",
       "  '332.500000',\n",
       "  '337.140015',\n",
       "  '337.140015',\n",
       "  '6069200'],\n",
       " ['2019-11-11',\n",
       "  '343.950012',\n",
       "  '349.190002',\n",
       "  '342.000000',\n",
       "  '345.089996',\n",
       "  '345.089996',\n",
       "  '9986700'],\n",
       " ['2019-11-12',\n",
       "  '346.899994',\n",
       "  '350.369995',\n",
       "  '344.040009',\n",
       "  '349.929993',\n",
       "  '349.929993',\n",
       "  '7359400'],\n",
       " ['2019-11-13',\n",
       "  '355.000000',\n",
       "  '356.329987',\n",
       "  '345.179993',\n",
       "  '346.109985',\n",
       "  '346.109985',\n",
       "  '8420100'],\n",
       " ['2019-11-14',\n",
       "  '346.109985',\n",
       "  '353.839996',\n",
       "  '342.910004',\n",
       "  '349.350006',\n",
       "  '349.350006',\n",
       "  '6464900'],\n",
       " ['2019-11-15',\n",
       "  '350.640015',\n",
       "  '352.799988',\n",
       "  '348.359985',\n",
       "  '352.170013',\n",
       "  '352.170013',\n",
       "  '4809000'],\n",
       " ['2019-11-18',\n",
       "  '352.920013',\n",
       "  '353.149994',\n",
       "  '346.100006',\n",
       "  '349.989990',\n",
       "  '349.989990',\n",
       "  '4400400'],\n",
       " ['2019-11-19',\n",
       "  '351.750000',\n",
       "  '359.989990',\n",
       "  '347.799988',\n",
       "  '359.519989',\n",
       "  '359.519989',\n",
       "  '7724800'],\n",
       " ['2019-11-20',\n",
       "  '360.000000',\n",
       "  '361.200012',\n",
       "  '349.570007',\n",
       "  '352.220001',\n",
       "  '352.220001',\n",
       "  '6725100'],\n",
       " ['2019-11-21',\n",
       "  '354.510010',\n",
       "  '360.839996',\n",
       "  '354.000000',\n",
       "  '354.829987',\n",
       "  '354.829987',\n",
       "  '6110000'],\n",
       " ['2019-11-22',\n",
       "  '340.160004',\n",
       "  '341.000000',\n",
       "  '330.000000',\n",
       "  '333.040009',\n",
       "  '333.040009',\n",
       "  '16870600'],\n",
       " ['2019-11-25',\n",
       "  '344.320007',\n",
       "  '344.570007',\n",
       "  '334.459991',\n",
       "  '336.339996',\n",
       "  '336.339996',\n",
       "  '12339500'],\n",
       " ['2019-11-26',\n",
       "  '335.269989',\n",
       "  '335.500000',\n",
       "  '327.100006',\n",
       "  '328.920013',\n",
       "  '328.920013',\n",
       "  '7947400'],\n",
       " ['2019-11-27',\n",
       "  '331.119995',\n",
       "  '333.929993',\n",
       "  '328.570007',\n",
       "  '331.290009',\n",
       "  '331.290009',\n",
       "  '5555600'],\n",
       " ['2019-11-29',\n",
       "  '331.109985',\n",
       "  '331.260010',\n",
       "  '327.500000',\n",
       "  '329.940002',\n",
       "  '329.940002',\n",
       "  '2465600'],\n",
       " ['2019-12-02',\n",
       "  '329.399994',\n",
       "  '336.380005',\n",
       "  '328.690002',\n",
       "  '334.869995',\n",
       "  '334.869995',\n",
       "  '6074500'],\n",
       " ['2019-12-03',\n",
       "  '332.619995',\n",
       "  '337.910004',\n",
       "  '332.190002',\n",
       "  '336.200012',\n",
       "  '336.200012',\n",
       "  '6573700'],\n",
       " ['2019-12-04',\n",
       "  '337.750000',\n",
       "  '337.859985',\n",
       "  '332.850006',\n",
       "  '333.029999',\n",
       "  '333.029999',\n",
       "  '5533000'],\n",
       " ['2019-12-05',\n",
       "  '332.829987',\n",
       "  '334.420013',\n",
       "  '327.250000',\n",
       "  '330.369995',\n",
       "  '330.369995',\n",
       "  '3724600'],\n",
       " ['2019-12-06',\n",
       "  '335.000000',\n",
       "  '338.859985',\n",
       "  '334.769989',\n",
       "  '335.890015',\n",
       "  '335.890015',\n",
       "  '7612400'],\n",
       " ['2019-12-09',\n",
       "  '336.589996',\n",
       "  '344.450012',\n",
       "  '335.079987',\n",
       "  '339.529999',\n",
       "  '339.529999',\n",
       "  '9023100'],\n",
       " ['2019-12-10',\n",
       "  '339.959991',\n",
       "  '350.730011',\n",
       "  '339.309998',\n",
       "  '348.839996',\n",
       "  '348.839996',\n",
       "  '8828300'],\n",
       " ['2019-12-11',\n",
       "  '351.880005',\n",
       "  '357.190002',\n",
       "  '351.089996',\n",
       "  '352.700012',\n",
       "  '352.700012',\n",
       "  '6897800'],\n",
       " ['2019-12-12',\n",
       "  '354.920013',\n",
       "  '362.739990',\n",
       "  '353.230011',\n",
       "  '359.679993',\n",
       "  '359.679993',\n",
       "  '7763900'],\n",
       " ['2019-12-13',\n",
       "  '361.049988',\n",
       "  '365.209991',\n",
       "  '354.640015',\n",
       "  '358.390015',\n",
       "  '358.390015',\n",
       "  '6570900'],\n",
       " ['2019-12-16',\n",
       "  '362.549988',\n",
       "  '383.609985',\n",
       "  '362.500000',\n",
       "  '381.500000',\n",
       "  '381.500000',\n",
       "  '18174200'],\n",
       " ['2019-12-17',\n",
       "  '378.989990',\n",
       "  '385.500000',\n",
       "  '375.899994',\n",
       "  '378.989990',\n",
       "  '378.989990',\n",
       "  '8496800'],\n",
       " ['2019-12-18',\n",
       "  '380.630005',\n",
       "  '395.220001',\n",
       "  '380.579987',\n",
       "  '393.149994',\n",
       "  '393.149994',\n",
       "  '14121000'],\n",
       " ['2019-12-19',\n",
       "  '397.320007',\n",
       "  '406.850006',\n",
       "  '396.500000',\n",
       "  '404.040009',\n",
       "  '404.040009',\n",
       "  '18107100'],\n",
       " ['2019-12-20',\n",
       "  '410.290009',\n",
       "  '413.000000',\n",
       "  '400.190002',\n",
       "  '405.589996',\n",
       "  '405.589996',\n",
       "  '14752700'],\n",
       " ['2019-12-23',\n",
       "  '411.779999',\n",
       "  '422.010010',\n",
       "  '410.000000',\n",
       "  '419.220001',\n",
       "  '419.220001',\n",
       "  '13319600'],\n",
       " ['2019-12-24',\n",
       "  '418.359985',\n",
       "  '425.470001',\n",
       "  '412.690002',\n",
       "  '425.250000',\n",
       "  '425.250000',\n",
       "  '8054700'],\n",
       " ['2019-12-26',\n",
       "  '427.910004',\n",
       "  '433.480011',\n",
       "  '426.350006',\n",
       "  '430.940002',\n",
       "  '430.940002',\n",
       "  '10633900'],\n",
       " ['2019-12-27',\n",
       "  '435.000000',\n",
       "  '435.309998',\n",
       "  '426.109985',\n",
       "  '430.380005',\n",
       "  '430.380005',\n",
       "  '9945700'],\n",
       " ['2019-12-30',\n",
       "  '428.790009',\n",
       "  '429.000000',\n",
       "  '409.260010',\n",
       "  '414.700012',\n",
       "  '414.700012',\n",
       "  '12586400'],\n",
       " ['2019-12-31',\n",
       "  '405.000000',\n",
       "  '421.290009',\n",
       "  '402.079987',\n",
       "  '418.329987',\n",
       "  '418.329987',\n",
       "  '10285700'],\n",
       " ['2020-01-02',\n",
       "  '424.500000',\n",
       "  '430.700012',\n",
       "  '421.709991',\n",
       "  '430.260010',\n",
       "  '430.260010',\n",
       "  '9532100'],\n",
       " ['2020-01-03',\n",
       "  '440.500000',\n",
       "  '454.000000',\n",
       "  '436.920013',\n",
       "  '443.010010',\n",
       "  '443.010010',\n",
       "  '17778500'],\n",
       " ['2020-01-06',\n",
       "  '440.470001',\n",
       "  '451.559998',\n",
       "  '440.000000',\n",
       "  '451.540009',\n",
       "  '451.540009',\n",
       "  '10133000'],\n",
       " ['2020-01-07',\n",
       "  '461.399994',\n",
       "  '471.630005',\n",
       "  '453.359985',\n",
       "  '469.059998',\n",
       "  '469.059998',\n",
       "  '17882100'],\n",
       " ['2020-01-08',\n",
       "  '473.700012',\n",
       "  '498.489990',\n",
       "  '468.230011',\n",
       "  '492.140015',\n",
       "  '492.140015',\n",
       "  '31144300'],\n",
       " ['2020-01-09',\n",
       "  '497.100006',\n",
       "  '498.799988',\n",
       "  '472.869995',\n",
       "  '481.339996',\n",
       "  '481.339996',\n",
       "  '28440400'],\n",
       " ['2020-01-10',\n",
       "  '481.790009',\n",
       "  '484.940002',\n",
       "  '473.700012',\n",
       "  '478.149994',\n",
       "  '478.149994',\n",
       "  '12959500'],\n",
       " ['2020-01-13',\n",
       "  '493.500000',\n",
       "  '525.630005',\n",
       "  '492.000000',\n",
       "  '524.859985',\n",
       "  '524.859985',\n",
       "  '26517600'],\n",
       " ['2020-01-14',\n",
       "  '544.260010',\n",
       "  '547.409973',\n",
       "  '524.900024',\n",
       "  '537.919983',\n",
       "  '537.919983',\n",
       "  '28996200'],\n",
       " ['2020-01-15',\n",
       "  '529.760010',\n",
       "  '537.840027',\n",
       "  '516.789978',\n",
       "  '518.500000',\n",
       "  '518.500000',\n",
       "  '17368800'],\n",
       " ['2020-01-16',\n",
       "  '493.750000',\n",
       "  '514.460022',\n",
       "  '492.170013',\n",
       "  '513.489990',\n",
       "  '513.489990',\n",
       "  '21736700'],\n",
       " ['2020-01-17',\n",
       "  '507.609985',\n",
       "  '515.669983',\n",
       "  '503.160004',\n",
       "  '510.500000',\n",
       "  '510.500000',\n",
       "  '13629100'],\n",
       " ['2020-01-21',\n",
       "  '530.250000',\n",
       "  '548.580017',\n",
       "  '528.409973',\n",
       "  '547.200012',\n",
       "  '547.200012',\n",
       "  '17803500'],\n",
       " ['2020-01-22',\n",
       "  '571.890015',\n",
       "  '594.500000',\n",
       "  '559.099976',\n",
       "  '569.559998',\n",
       "  '569.559998',\n",
       "  '31369000'],\n",
       " ['2020-01-23',\n",
       "  '564.250000',\n",
       "  '582.000000',\n",
       "  '555.599976',\n",
       "  '572.200012',\n",
       "  '572.200012',\n",
       "  '19651000'],\n",
       " ['2020-01-24',\n",
       "  '570.630005',\n",
       "  '573.859985',\n",
       "  '554.260010',\n",
       "  '564.820007',\n",
       "  '564.820007',\n",
       "  '14353600'],\n",
       " ['2020-01-27',\n",
       "  '541.989990',\n",
       "  '564.440002',\n",
       "  '539.280029',\n",
       "  '558.020020',\n",
       "  '558.020020',\n",
       "  '13608100'],\n",
       " ['2020-01-28',\n",
       "  '568.489990',\n",
       "  '576.809998',\n",
       "  '558.080017',\n",
       "  '566.900024',\n",
       "  '566.900024',\n",
       "  '11788500'],\n",
       " ['2020-01-29',\n",
       "  '575.690002',\n",
       "  '589.799988',\n",
       "  '567.429993',\n",
       "  '580.989990',\n",
       "  '580.989990',\n",
       "  '17801500'],\n",
       " ['2020-01-30',\n",
       "  '632.419983',\n",
       "  '650.880005',\n",
       "  '618.000000',\n",
       "  '640.809998',\n",
       "  '640.809998',\n",
       "  '29005700'],\n",
       " ['2020-01-31',\n",
       "  '640.000000',\n",
       "  '653.000000',\n",
       "  '632.520020',\n",
       "  '650.570007',\n",
       "  '650.570007',\n",
       "  '15719300'],\n",
       " ['2020-02-03',\n",
       "  '673.690002',\n",
       "  '786.140015',\n",
       "  '673.520020',\n",
       "  '780.000000',\n",
       "  '780.000000',\n",
       "  '47233500'],\n",
       " ['2020-02-04',\n",
       "  '882.960022',\n",
       "  '968.989990',\n",
       "  '833.880005',\n",
       "  '887.059998',\n",
       "  '887.059998',\n",
       "  '60938800'],\n",
       " ['2020-02-05',\n",
       "  '823.260010',\n",
       "  '845.979980',\n",
       "  '704.109985',\n",
       "  '734.700012',\n",
       "  '734.700012',\n",
       "  '48423800'],\n",
       " ['2020-02-06',\n",
       "  '699.919983',\n",
       "  '795.830017',\n",
       "  '687.000000',\n",
       "  '748.960022',\n",
       "  '748.960022',\n",
       "  '39880800'],\n",
       " ['2020-02-07',\n",
       "  '730.549988',\n",
       "  '769.750000',\n",
       "  '730.000000',\n",
       "  '748.070007',\n",
       "  '748.070007',\n",
       "  '17063500'],\n",
       " ['2020-02-10',\n",
       "  '800.000000',\n",
       "  '819.989990',\n",
       "  '752.400024',\n",
       "  '771.280029',\n",
       "  '771.280029',\n",
       "  '24689200'],\n",
       " ['2020-02-11',\n",
       "  '768.789978',\n",
       "  '783.510010',\n",
       "  '758.000000',\n",
       "  '774.380005',\n",
       "  '774.380005',\n",
       "  '11697500'],\n",
       " ['2020-02-12',\n",
       "  '777.869995',\n",
       "  '789.750000',\n",
       "  '763.369995',\n",
       "  '767.289978',\n",
       "  '767.289978',\n",
       "  '12022500'],\n",
       " ['2020-02-13',\n",
       "  '741.840027',\n",
       "  '818.000000',\n",
       "  '735.000000',\n",
       "  '804.000000',\n",
       "  '804.000000',\n",
       "  '26289300'],\n",
       " ['2020-02-14',\n",
       "  '787.219971',\n",
       "  '812.969971',\n",
       "  '785.500000',\n",
       "  '800.030029',\n",
       "  '800.030029',\n",
       "  '15693700'],\n",
       " ['2020-02-18',\n",
       "  '841.599976',\n",
       "  '860.000000',\n",
       "  '832.359985',\n",
       "  '858.400024',\n",
       "  '858.400024',\n",
       "  '16381700'],\n",
       " ['2020-02-19',\n",
       "  '923.500000',\n",
       "  '944.780029',\n",
       "  '901.020020',\n",
       "  '917.419983',\n",
       "  '917.419983',\n",
       "  '25423000'],\n",
       " ['2020-02-20',\n",
       "  '911.950012',\n",
       "  '912.000000',\n",
       "  '859.940002',\n",
       "  '899.409973',\n",
       "  '899.409973',\n",
       "  '17634900'],\n",
       " ['2020-02-21',\n",
       "  '906.979980',\n",
       "  '913.059998',\n",
       "  '880.450012',\n",
       "  '901.000000',\n",
       "  '901.000000',\n",
       "  '14314800'],\n",
       " ['2020-02-24',\n",
       "  '839.000000',\n",
       "  '863.500000',\n",
       "  '822.200012',\n",
       "  '833.789978',\n",
       "  '833.789978',\n",
       "  '15192200'],\n",
       " ['2020-02-25',\n",
       "  '849.000000',\n",
       "  '856.599976',\n",
       "  '787.000000',\n",
       "  '799.909973',\n",
       "  '799.909973',\n",
       "  '17290500'],\n",
       " ['2020-02-26',\n",
       "  '782.500000',\n",
       "  '813.309998',\n",
       "  '776.109985',\n",
       "  '778.799988',\n",
       "  '778.799988',\n",
       "  '14085500'],\n",
       " ['2020-02-27',\n",
       "  '730.000000',\n",
       "  '739.770020',\n",
       "  '669.000000',\n",
       "  '679.000000',\n",
       "  '679.000000',\n",
       "  '24149300'],\n",
       " ['2020-02-28',\n",
       "  '629.700012',\n",
       "  '690.520020',\n",
       "  '611.520020',\n",
       "  '667.989990',\n",
       "  '667.989990',\n",
       "  '24564200'],\n",
       " ['2020-03-02',\n",
       "  '711.260010',\n",
       "  '743.690002',\n",
       "  '686.669983',\n",
       "  '743.619995',\n",
       "  '743.619995',\n",
       "  '20195000'],\n",
       " ['2020-03-03',\n",
       "  '805.000000',\n",
       "  '806.979980',\n",
       "  '716.109985',\n",
       "  '745.510010',\n",
       "  '745.510010',\n",
       "  '25784000'],\n",
       " ['2020-03-04',\n",
       "  '763.960022',\n",
       "  '766.520020',\n",
       "  '724.729980',\n",
       "  '749.500000',\n",
       "  '749.500000',\n",
       "  '15049000'],\n",
       " ['2020-03-05',\n",
       "  '723.770020',\n",
       "  '745.750000',\n",
       "  '718.070007',\n",
       "  '724.539978',\n",
       "  '724.539978',\n",
       "  '10852700'],\n",
       " ['2020-03-06',\n",
       "  '690.000000',\n",
       "  '707.000000',\n",
       "  '684.270020',\n",
       "  '703.479980',\n",
       "  '703.479980',\n",
       "  '12662900'],\n",
       " ['2020-03-09',\n",
       "  '605.390015',\n",
       "  '663.000000',\n",
       "  '605.000000',\n",
       "  '608.000000',\n",
       "  '608.000000',\n",
       "  '17073700'],\n",
       " ['2020-03-10',\n",
       "  '659.429993',\n",
       "  '668.000000',\n",
       "  '608.000000',\n",
       "  '645.330017',\n",
       "  '645.330017',\n",
       "  '15594400'],\n",
       " ['2020-03-11',\n",
       "  '640.200012',\n",
       "  '653.580017',\n",
       "  '613.000000',\n",
       "  '634.229980',\n",
       "  '634.229980',\n",
       "  '13322500'],\n",
       " ['2020-03-12',\n",
       "  '580.890015',\n",
       "  '594.500000',\n",
       "  '546.250000',\n",
       "  '560.549988',\n",
       "  '560.549988',\n",
       "  '18909100'],\n",
       " ['2020-03-13',\n",
       "  '595.000000',\n",
       "  '607.570007',\n",
       "  '502.000000',\n",
       "  '546.619995',\n",
       "  '546.619995',\n",
       "  '22640300'],\n",
       " ['2020-03-16',\n",
       "  '469.500000',\n",
       "  '494.869995',\n",
       "  '442.170013',\n",
       "  '445.070007',\n",
       "  '445.070007',\n",
       "  '20489500'],\n",
       " ['2020-03-17',\n",
       "  '440.010010',\n",
       "  '471.850006',\n",
       "  '396.000000',\n",
       "  '430.200012',\n",
       "  '430.200012',\n",
       "  '23994600'],\n",
       " ['2020-03-18',\n",
       "  '389.000000',\n",
       "  '404.859985',\n",
       "  '350.510010',\n",
       "  '361.220001',\n",
       "  '361.220001',\n",
       "  '23786200'],\n",
       " ['2020-03-19',\n",
       "  '374.700012',\n",
       "  '452.000000',\n",
       "  '358.459991',\n",
       "  '427.640015',\n",
       "  '427.640015',\n",
       "  '30195500'],\n",
       " ['2020-03-20',\n",
       "  '438.200012',\n",
       "  '477.000000',\n",
       "  '425.790009',\n",
       "  '427.529999',\n",
       "  '427.529999',\n",
       "  '28285500'],\n",
       " ['2020-03-23',\n",
       "  '433.600006',\n",
       "  '442.000000',\n",
       "  '410.500000',\n",
       "  '434.290009',\n",
       "  '434.290009',\n",
       "  '16454500'],\n",
       " ['2020-03-24',\n",
       "  '477.299988',\n",
       "  '513.690002',\n",
       "  '474.000000',\n",
       "  '505.000000',\n",
       "  '505.000000',\n",
       "  '22895200'],\n",
       " ['2020-03-25',\n",
       "  '545.250000',\n",
       "  '557.000000',\n",
       "  '511.109985',\n",
       "  '539.250000',\n",
       "  '539.250000',\n",
       "  '21222700'],\n",
       " ['2020-03-26',\n",
       "  '547.390015',\n",
       "  '560.000000',\n",
       "  '512.250000',\n",
       "  '528.159973',\n",
       "  '528.159973',\n",
       "  '17380700'],\n",
       " ['2020-03-27',\n",
       "  '505.000000',\n",
       "  '525.799988',\n",
       "  '494.029999',\n",
       "  '514.359985',\n",
       "  '514.359985',\n",
       "  '14377400'],\n",
       " ['2020-03-30',\n",
       "  '510.260010',\n",
       "  '516.650024',\n",
       "  '491.230011',\n",
       "  '502.130005',\n",
       "  '502.130005',\n",
       "  '11998100'],\n",
       " ['2020-03-31',\n",
       "  '501.250000',\n",
       "  '542.960022',\n",
       "  '497.000000',\n",
       "  '524.000000',\n",
       "  '524.000000',\n",
       "  '17771500'],\n",
       " ['2020-04-01',\n",
       "  '504.000000',\n",
       "  '513.950012',\n",
       "  '475.100006',\n",
       "  '481.559998',\n",
       "  '481.559998',\n",
       "  '13353200'],\n",
       " ['2020-04-02',\n",
       "  '481.029999',\n",
       "  '494.260010',\n",
       "  '446.399994',\n",
       "  '454.470001',\n",
       "  '454.470001',\n",
       "  '19858400'],\n",
       " ['2020-04-03',\n",
       "  '509.500000',\n",
       "  '515.489990',\n",
       "  '468.390015',\n",
       "  '480.010010',\n",
       "  '480.010010',\n",
       "  '22562100'],\n",
       " ['2020-04-06',\n",
       "  '511.200012',\n",
       "  '521.000000',\n",
       "  '497.959991',\n",
       "  '516.239990',\n",
       "  '516.239990',\n",
       "  '14901800'],\n",
       " ['2020-04-07',\n",
       "  '545.000000',\n",
       "  '565.000000',\n",
       "  '532.340027',\n",
       "  '545.450012',\n",
       "  '545.450012',\n",
       "  '17919800'],\n",
       " ['2020-04-08',\n",
       "  '554.200012',\n",
       "  '557.210022',\n",
       "  '533.330017',\n",
       "  '548.840027',\n",
       "  '548.840027',\n",
       "  '12656000'],\n",
       " ['2020-04-09',\n",
       "  '562.090027',\n",
       "  '575.179993',\n",
       "  '557.109985',\n",
       "  '573.000000',\n",
       "  '573.000000',\n",
       "  '13650000'],\n",
       " ['2020-04-13',\n",
       "  '590.159973',\n",
       "  '652.000000',\n",
       "  '580.530029',\n",
       "  '650.950012',\n",
       "  '650.950012',\n",
       "  '22475400'],\n",
       " ['2020-04-14',\n",
       "  '698.969971',\n",
       "  '741.880005',\n",
       "  '692.429993',\n",
       "  '709.890015',\n",
       "  '709.890015',\n",
       "  '30576500'],\n",
       " ['2020-04-15',\n",
       "  '742.000000',\n",
       "  '753.130005',\n",
       "  '710.000000',\n",
       "  '729.830017',\n",
       "  '729.830017',\n",
       "  '23577000'],\n",
       " ['2020-04-16',\n",
       "  '716.940002',\n",
       "  '759.450012',\n",
       "  '706.719971',\n",
       "  '745.210022',\n",
       "  '745.210022',\n",
       "  '20657900'],\n",
       " ['2020-04-17',\n",
       "  '772.280029',\n",
       "  '774.950012',\n",
       "  '747.659973',\n",
       "  '753.890015',\n",
       "  '753.890015',\n",
       "  '13128200'],\n",
       " ['2020-04-20',\n",
       "  '732.700012',\n",
       "  '765.570007',\n",
       "  '712.210022',\n",
       "  '746.359985',\n",
       "  '746.359985',\n",
       "  '14746600'],\n",
       " ['2020-04-21',\n",
       "  '730.119995',\n",
       "  '753.330017',\n",
       "  '673.789978',\n",
       "  '686.719971',\n",
       "  '686.719971',\n",
       "  '20209100'],\n",
       " ['2020-04-22',\n",
       "  '703.979980',\n",
       "  '734.000000',\n",
       "  '688.710022',\n",
       "  '732.109985',\n",
       "  '732.109985',\n",
       "  '14224800'],\n",
       " ['2020-04-23',\n",
       "  '727.599976',\n",
       "  '734.000000',\n",
       "  '703.130005',\n",
       "  '705.630005',\n",
       "  '705.630005',\n",
       "  '13236700'],\n",
       " ['2020-04-24',\n",
       "  '710.809998',\n",
       "  '730.729980',\n",
       "  '698.179993',\n",
       "  '725.150024',\n",
       "  '725.150024',\n",
       "  '13237600'],\n",
       " ['2020-04-27',\n",
       "  '737.609985',\n",
       "  '799.489990',\n",
       "  '735.000000',\n",
       "  '798.750000',\n",
       "  '798.750000',\n",
       "  '20681400'],\n",
       " ['2020-04-28',\n",
       "  '795.640015',\n",
       "  '805.000000',\n",
       "  '756.690002',\n",
       "  '769.119995',\n",
       "  '769.119995',\n",
       "  '15222000'],\n",
       " ['2020-04-29',\n",
       "  '790.169983',\n",
       "  '803.200012',\n",
       "  '783.159973',\n",
       "  '800.510010',\n",
       "  '800.510010',\n",
       "  '16216000'],\n",
       " ['2020-04-30',\n",
       "  '855.190002',\n",
       "  '869.820007',\n",
       "  '763.500000',\n",
       "  '781.880005',\n",
       "  '781.880005',\n",
       "  '28400100'],\n",
       " ['2020-05-01',\n",
       "  '755.000000',\n",
       "  '772.770020',\n",
       "  '683.039978',\n",
       "  '701.320007',\n",
       "  '701.320007',\n",
       "  '32531800'],\n",
       " ['2020-05-04',\n",
       "  '701.000000',\n",
       "  '762.000000',\n",
       "  '698.000000',\n",
       "  '761.190002',\n",
       "  '761.190002',\n",
       "  '19237100'],\n",
       " ['2020-05-05',\n",
       "  '789.789978',\n",
       "  '798.919983',\n",
       "  '762.179993',\n",
       "  '768.210022',\n",
       "  '768.210022',\n",
       "  '16991700'],\n",
       " ['2020-05-06',\n",
       "  '776.500000',\n",
       "  '789.799988',\n",
       "  '761.109985',\n",
       "  '782.580017',\n",
       "  '782.580017',\n",
       "  '11123200'],\n",
       " ['2020-05-07',\n",
       "  '777.210022',\n",
       "  '796.400024',\n",
       "  '772.349976',\n",
       "  '780.039978',\n",
       "  '780.039978',\n",
       "  '11527700'],\n",
       " ['2020-05-08',\n",
       "  '793.770020',\n",
       "  '824.000000',\n",
       "  '787.010010',\n",
       "  '819.419983',\n",
       "  '819.419983',\n",
       "  '16130100'],\n",
       " ['2020-05-11',\n",
       "  '790.510010',\n",
       "  '824.000000',\n",
       "  '785.000000',\n",
       "  '811.289978',\n",
       "  '811.289978',\n",
       "  '16471100'],\n",
       " ['2020-05-12',\n",
       "  '827.000000',\n",
       "  '843.289978',\n",
       "  '808.000000',\n",
       "  '809.409973',\n",
       "  '809.409973',\n",
       "  '15906900'],\n",
       " ['2020-05-13',\n",
       "  '820.830017',\n",
       "  '826.000000',\n",
       "  '763.299988',\n",
       "  '790.960022',\n",
       "  '790.960022',\n",
       "  '19065500'],\n",
       " ['2020-05-14',\n",
       "  '780.000000',\n",
       "  '803.359985',\n",
       "  '764.000000',\n",
       "  '803.330017',\n",
       "  '803.330017',\n",
       "  '13682200'],\n",
       " ['2020-05-15',\n",
       "  '790.349976',\n",
       "  '805.049988',\n",
       "  '786.549988',\n",
       "  '799.169983',\n",
       "  '799.169983',\n",
       "  '10518400'],\n",
       " ['2020-05-18',\n",
       "  '827.780029',\n",
       "  '834.719971',\n",
       "  '803.880005',\n",
       "  '813.630005',\n",
       "  '813.630005',\n",
       "  '11698100'],\n",
       " ['2020-05-19',\n",
       "  '815.169983',\n",
       "  '822.070007',\n",
       "  '806.080017',\n",
       "  '808.010010',\n",
       "  '808.010010',\n",
       "  '9636500'],\n",
       " ['2020-05-20',\n",
       "  '820.500000',\n",
       "  '826.000000',\n",
       "  '811.799988',\n",
       "  '815.559998',\n",
       "  '815.559998',\n",
       "  '7309300'],\n",
       " ['2020-05-21',\n",
       "  '816.000000',\n",
       "  '832.500000',\n",
       "  '796.000000',\n",
       "  '827.599976',\n",
       "  '827.599976',\n",
       "  '12254600'],\n",
       " ['2020-05-22',\n",
       "  '822.169983',\n",
       "  '831.780029',\n",
       "  '812.000000',\n",
       "  '816.880005',\n",
       "  '816.880005',\n",
       "  '9987500'],\n",
       " ['2020-05-26',\n",
       "  '834.500000',\n",
       "  '834.599976',\n",
       "  '815.710022',\n",
       "  '818.869995',\n",
       "  '818.869995',\n",
       "  '8089700'],\n",
       " ['2020-05-27',\n",
       "  '820.859985',\n",
       "  '827.710022',\n",
       "  '785.000000',\n",
       "  '820.229980',\n",
       "  '820.229980',\n",
       "  '11549500'],\n",
       " ['2020-05-28',\n",
       "  '813.510010',\n",
       "  '824.750000',\n",
       "  '801.690002',\n",
       "  '805.809998',\n",
       "  '805.809998',\n",
       "  '7255600'],\n",
       " ['2020-05-29',\n",
       "  '808.750000',\n",
       "  '835.000000',\n",
       "  '804.210022',\n",
       "  '835.000000',\n",
       "  '835.000000',\n",
       "  '11812500'],\n",
       " ['2020-06-01',\n",
       "  '858.000000',\n",
       "  '899.000000',\n",
       "  '854.099976',\n",
       "  '898.099976',\n",
       "  '898.099976',\n",
       "  '14939500'],\n",
       " ['2020-06-02',\n",
       "  '894.700012',\n",
       "  '908.659973',\n",
       "  '871.000000',\n",
       "  '881.559998',\n",
       "  '881.559998',\n",
       "  '13565600'],\n",
       " ['2020-06-03',\n",
       "  '888.119995',\n",
       "  '897.940002',\n",
       "  '880.099976',\n",
       "  '882.960022',\n",
       "  '882.960022',\n",
       "  '7949500'],\n",
       " ['2020-06-04',\n",
       "  '889.880005',\n",
       "  '895.750000',\n",
       "  '858.440002',\n",
       "  '864.380005',\n",
       "  '864.380005',\n",
       "  '8887700'],\n",
       " ['2020-06-05',\n",
       "  '877.840027',\n",
       "  '886.520020',\n",
       "  '866.200012',\n",
       "  '885.659973',\n",
       "  '885.659973',\n",
       "  '7811900'],\n",
       " ['2020-06-08',\n",
       "  '919.000000',\n",
       "  '950.000000',\n",
       "  '909.159973',\n",
       "  '949.919983',\n",
       "  '949.919983',\n",
       "  '14174700'],\n",
       " ['2020-06-09',\n",
       "  '940.010010',\n",
       "  '954.440002',\n",
       "  '923.929993',\n",
       "  '940.669983',\n",
       "  '940.669983',\n",
       "  '11388200'],\n",
       " ['2020-06-10',\n",
       "  '991.880005',\n",
       "  '1027.479980',\n",
       "  '982.500000',\n",
       "  '1025.050049',\n",
       "  '1025.050049',\n",
       "  '18563400'],\n",
       " ['2020-06-11',\n",
       "  '990.200012',\n",
       "  '1018.960022',\n",
       "  '972.000000',\n",
       "  '972.840027',\n",
       "  '972.840027',\n",
       "  '15916500'],\n",
       " ['2020-06-12',\n",
       "  '980.000000',\n",
       "  '987.979980',\n",
       "  '912.599976',\n",
       "  '935.280029',\n",
       "  '935.280029',\n",
       "  '16730200'],\n",
       " ['2020-06-15',\n",
       "  '917.789978',\n",
       "  '998.840027',\n",
       "  '908.500000',\n",
       "  '990.900024',\n",
       "  '990.900024',\n",
       "  '15697200'],\n",
       " ['2020-06-16',\n",
       "  '1011.849976',\n",
       "  '1012.880005',\n",
       "  '962.390015',\n",
       "  '982.130005',\n",
       "  '982.130005',\n",
       "  '14051100'],\n",
       " ['2020-06-17',\n",
       "  '987.710022',\n",
       "  '1005.000000',\n",
       "  '982.570007',\n",
       "  '991.789978',\n",
       "  '991.789978',\n",
       "  '9869400'],\n",
       " ['2020-06-18',\n",
       "  '1003.000000',\n",
       "  '1019.200012',\n",
       "  '994.469971',\n",
       "  '1003.960022',\n",
       "  '1003.960022',\n",
       "  '9751900'],\n",
       " ['2020-06-19',\n",
       "  '1012.780029',\n",
       "  '1015.969971',\n",
       "  '991.340027',\n",
       "  '1000.900024',\n",
       "  '1000.900024',\n",
       "  '8679700'],\n",
       " ['2020-06-22',\n",
       "  '999.950012',\n",
       "  '1008.880005',\n",
       "  '990.020020',\n",
       "  '994.320007',\n",
       "  '994.320007',\n",
       "  '6362400'],\n",
       " ['2020-06-23',\n",
       "  '998.880005',\n",
       "  '1012.000000',\n",
       "  '994.010010',\n",
       "  '1001.780029',\n",
       "  '1001.780029',\n",
       "  '6365300'],\n",
       " ['2020-06-24',\n",
       "  '994.109985',\n",
       "  '1000.880005',\n",
       "  '953.140015',\n",
       "  '960.849976',\n",
       "  '960.849976',\n",
       "  '10959600'],\n",
       " ['2020-06-25',\n",
       "  '954.270020',\n",
       "  '985.979980',\n",
       "  '937.150024',\n",
       "  '985.979980',\n",
       "  '985.979980',\n",
       "  '9254500'],\n",
       " ['2020-06-26',\n",
       "  '994.780029',\n",
       "  '995.000000',\n",
       "  '954.869995',\n",
       "  '959.739990',\n",
       "  '959.739990',\n",
       "  '8854900'],\n",
       " ['2020-06-29',\n",
       "  '969.010010',\n",
       "  '1010.000000',\n",
       "  '948.520020',\n",
       "  '1009.349976',\n",
       "  '1009.349976',\n",
       "  '9026400'],\n",
       " ['2020-06-30',\n",
       "  '1006.500000',\n",
       "  '1087.689941',\n",
       "  '1003.729980',\n",
       "  '1079.810059',\n",
       "  '1079.810059',\n",
       "  '16918500'],\n",
       " ['2020-07-01',\n",
       "  '1083.000000',\n",
       "  '1135.329956',\n",
       "  '1080.500000',\n",
       "  '1119.630005',\n",
       "  '1119.630005',\n",
       "  '13326900'],\n",
       " ['2020-07-02',\n",
       "  '1221.479980',\n",
       "  '1228.000000',\n",
       "  '1185.599976',\n",
       "  '1208.660034',\n",
       "  '1208.660034',\n",
       "  '17250100'],\n",
       " ['2020-07-06',\n",
       "  '1276.689941',\n",
       "  '1377.790039',\n",
       "  '1266.040039',\n",
       "  '1371.579956',\n",
       "  '1371.579956',\n",
       "  '20569900'],\n",
       " ['2020-07-07',\n",
       "  '1405.010010',\n",
       "  '1429.500000',\n",
       "  '1336.709961',\n",
       "  '1389.859985',\n",
       "  '1389.859985',\n",
       "  '21489700'],\n",
       " ['2020-07-08',\n",
       "  '1405.000000',\n",
       "  '1417.260010',\n",
       "  '1311.339966',\n",
       "  '1365.880005',\n",
       "  '1365.880005',\n",
       "  '16311300'],\n",
       " ['2020-07-09',\n",
       "  '1396.989990',\n",
       "  '1408.560059',\n",
       "  '1351.280029',\n",
       "  '1394.280029',\n",
       "  '1394.280029',\n",
       "  '11717600'],\n",
       " ['2020-07-10',\n",
       "  '1396.000000',\n",
       "  '1548.920044',\n",
       "  '1376.010010',\n",
       "  '1544.650024',\n",
       "  '1544.650024',\n",
       "  '23337600'],\n",
       " ['2020-07-13',\n",
       "  '1659.000000',\n",
       "  '1794.989990',\n",
       "  '1471.109985',\n",
       "  '1497.060059',\n",
       "  '1497.060059',\n",
       "  '38725700'],\n",
       " ['2020-07-14',\n",
       "  '1556.000000',\n",
       "  '1590.000000',\n",
       "  '1431.000000',\n",
       "  '1516.800049',\n",
       "  '1516.800049',\n",
       "  '22833862']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Maryam', age=27)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li = [('Maryam', 27)]\n",
    "df = sqlContext.createDataFrame(li, ['name', 'age'])\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(date='Date', open='Open', high='High', low='Low', close='Close', adjclose='AdjClose', volume='Volume'),\n",
       " Row(date='2019-07-15', open='248.000000', high='254.419998', low='244.860001', close='253.500000', adjclose='253.500000', volume='11000100'),\n",
       " Row(date='2019-07-16', open='249.300003', high='253.529999', low='247.929993', close='252.380005', adjclose='252.380005', volume='8149000'),\n",
       " Row(date='2019-07-17', open='255.669998', high='258.309998', low='253.350006', close='254.860001', adjclose='254.860001', volume='9764700'),\n",
       " Row(date='2019-07-18', open='255.050003', high='255.750000', low='251.889999', close='253.539993', adjclose='253.539993', volume='4764500')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tesla_df = csv_rdd.toDF(['date', 'open', 'high', 'low', 'close', 'adjclose', 'volume'])\n",
    "tesla_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_file = 'AMZN.csv'\n",
    "amazon_df = sqlContext.read.load(amazon_file,\n",
    "                                format='com.databricks.spark.csv',\n",
    "                                header='true',\n",
    "                                inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date='2019-07-15', Open=2021.400024, High=2022.900024, Low=2001.550049, Close=2020.98999, AdjClose=2020.98999, Volume=2981300),\n",
       " Row(Date='2019-07-16', Open=2010.579956, High=2026.319946, Low=2001.219971, Close=2009.900024, AdjClose=2009.900024, Volume=2618200)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- AdjClose: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "amazon_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+-----------+-----------+-------+\n",
      "|      Date|       Open|       High|        Low|      Close|   AdjClose| Volume|\n",
      "+----------+-----------+-----------+-----------+-----------+-----------+-------+\n",
      "|2019-07-15|2021.400024|2022.900024|2001.550049| 2020.98999| 2020.98999|2981300|\n",
      "|2019-07-16|2010.579956|2026.319946|2001.219971|2009.900024|2009.900024|2618200|\n",
      "|2019-07-17|2007.050049|     2012.0|1992.030029|1992.030029|1992.030029|2558800|\n",
      "|2019-07-18| 1980.01001|     1987.5|1951.550049|1977.900024|1977.900024|3504300|\n",
      "|2019-07-19|1991.209961|     1996.0| 1962.22998| 1964.52002| 1964.52002|3185600|\n",
      "|2019-07-22|1971.140015|     1989.0| 1958.26001|1985.630005|1985.630005|2900000|\n",
      "|2019-07-23| 1995.98999|1997.790039|1973.130005| 1994.48999| 1994.48999|2703500|\n",
      "|2019-07-24|1969.300049|2001.300049|1965.869995|2000.810059|2000.810059|2631300|\n",
      "|2019-07-25|     2001.0|2001.199951|1972.719971|1973.819946|1973.819946|4136500|\n",
      "|2019-07-26|     1942.0|1950.900024| 1924.51001|1943.050049|1943.050049|4927100|\n",
      "|2019-07-29|     1930.0| 1932.22998|1890.540039|1912.449951|1912.449951|4493200|\n",
      "|2019-07-30|1891.119995|1909.890015| 1883.47998|1898.530029|1898.530029|2910900|\n",
      "|2019-07-31|1898.109985|1899.550049|1849.439941|1866.780029|1866.780029|4470700|\n",
      "|2019-08-01|1871.719971|1897.920044| 1844.01001|1855.319946|1855.319946|4713300|\n",
      "|2019-08-02|1845.069946|1846.359985| 1808.02002| 1823.23999| 1823.23999|4956200|\n",
      "|2019-08-05|1770.219971|1788.670044|1748.780029|1765.130005|1765.130005|6058200|\n",
      "|2019-08-06| 1792.22998| 1793.77002|1753.400024|1787.829956|1787.829956|5070300|\n",
      "|2019-08-07| 1773.98999|1798.930054|     1757.0|1793.400024|1793.400024|4526900|\n",
      "|2019-08-08|     1806.0| 1834.26001|1798.109985|1832.890015|1832.890015|3701200|\n",
      "|2019-08-09|1828.949951|1831.089966|1802.219971|1807.579956|1807.579956|2879800|\n",
      "+----------+-----------+-----------+-----------+-----------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "amazon_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>AdjClose</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-07-15</td>\n",
       "      <td>2021.400024</td>\n",
       "      <td>2022.900024</td>\n",
       "      <td>2001.550049</td>\n",
       "      <td>2020.989990</td>\n",
       "      <td>2020.989990</td>\n",
       "      <td>2981300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-07-16</td>\n",
       "      <td>2010.579956</td>\n",
       "      <td>2026.319946</td>\n",
       "      <td>2001.219971</td>\n",
       "      <td>2009.900024</td>\n",
       "      <td>2009.900024</td>\n",
       "      <td>2618200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-07-17</td>\n",
       "      <td>2007.050049</td>\n",
       "      <td>2012.000000</td>\n",
       "      <td>1992.030029</td>\n",
       "      <td>1992.030029</td>\n",
       "      <td>1992.030029</td>\n",
       "      <td>2558800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-07-18</td>\n",
       "      <td>1980.010010</td>\n",
       "      <td>1987.500000</td>\n",
       "      <td>1951.550049</td>\n",
       "      <td>1977.900024</td>\n",
       "      <td>1977.900024</td>\n",
       "      <td>3504300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-07-19</td>\n",
       "      <td>1991.209961</td>\n",
       "      <td>1996.000000</td>\n",
       "      <td>1962.229980</td>\n",
       "      <td>1964.520020</td>\n",
       "      <td>1964.520020</td>\n",
       "      <td>3185600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date         Open         High          Low        Close  \\\n",
       "0  2019-07-15  2021.400024  2022.900024  2001.550049  2020.989990   \n",
       "1  2019-07-16  2010.579956  2026.319946  2001.219971  2009.900024   \n",
       "2  2019-07-17  2007.050049  2012.000000  1992.030029  1992.030029   \n",
       "3  2019-07-18  1980.010010  1987.500000  1951.550049  1977.900024   \n",
       "4  2019-07-19  1991.209961  1996.000000  1962.229980  1964.520020   \n",
       "\n",
       "      AdjClose   Volume  \n",
       "0  2020.989990  2981300  \n",
       "1  2009.900024  2618200  \n",
       "2  1992.030029  2558800  \n",
       "3  1977.900024  3504300  \n",
       "4  1964.520020  3185600  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "amazon_df.toPandas().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_file = 'GOOG.csv'\n",
    "google_df = sqlContext.read.load(google_file,\n",
    "                                format='com.databricks.spark.csv',\n",
    "                                header='true',\n",
    "                                inferSchema='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore and Query Data\n",
    "## DataFrames operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|  yr|        avg(Close)|\n",
      "+----+------------------+\n",
      "|2019|1245.3833654621849|\n",
      "|2020|1362.8286906865671|\n",
      "+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month\n",
    "\n",
    "google_df.select(year(\"Date\").alias(\"yr\"), \"Close\").groupby(\"yr\").avg(\"Close\").sort(\"yr\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DataFrame in module pyspark.sql.dataframe object:\n",
      "\n",
      "class DataFrame(pyspark.sql.pandas.map_ops.PandasMapOpsMixin, pyspark.sql.pandas.conversion.PandasConversionMixin)\n",
      " |  DataFrame(jdf, sql_ctx)\n",
      " |  \n",
      " |  A distributed collection of data grouped into named columns.\n",
      " |  \n",
      " |  A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n",
      " |  and can be created using various functions in :class:`SparkSession`::\n",
      " |  \n",
      " |      people = spark.read.parquet(\"...\")\n",
      " |  \n",
      " |  Once created, it can be manipulated using the various domain-specific-language\n",
      " |  (DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n",
      " |  \n",
      " |  To select a column from the :class:`DataFrame`, use the apply method::\n",
      " |  \n",
      " |      ageCol = people.age\n",
      " |  \n",
      " |  A more concrete example::\n",
      " |  \n",
      " |      # To create DataFrame using SparkSession\n",
      " |      people = spark.read.parquet(\"...\")\n",
      " |      department = spark.read.parquet(\"...\")\n",
      " |  \n",
      " |      people.filter(people.age > 30).join(department, people.deptId == department.id) \\\n",
      " |        .groupBy(department.name, \"gender\").agg({\"salary\": \"avg\", \"age\": \"max\"})\n",
      " |  \n",
      " |  .. versionadded:: 1.3\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DataFrame\n",
      " |      pyspark.sql.pandas.map_ops.PandasMapOpsMixin\n",
      " |      pyspark.sql.pandas.conversion.PandasConversionMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |      Returns the :class:`Column` denoted by ``name``.\n",
      " |      \n",
      " |      >>> df.select(df.age).collect()\n",
      " |      [Row(age=2), Row(age=5)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  __getitem__(self, item)\n",
      " |      Returns the column as a :class:`Column`.\n",
      " |      \n",
      " |      >>> df.select(df['age']).collect()\n",
      " |      [Row(age=2), Row(age=5)]\n",
      " |      >>> df[ [\"name\", \"age\"]].collect()\n",
      " |      [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      " |      >>> df[ df.age > 3 ].collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      >>> df[df[0] > 3].collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  __init__(self, jdf, sql_ctx)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  agg(self, *exprs)\n",
      " |      Aggregate on the entire :class:`DataFrame` without groups\n",
      " |      (shorthand for ``df.groupBy.agg()``).\n",
      " |      \n",
      " |      >>> df.agg({\"age\": \"max\"}).collect()\n",
      " |      [Row(max(age)=5)]\n",
      " |      >>> from pyspark.sql import functions as F\n",
      " |      >>> df.agg(F.min(df.age)).collect()\n",
      " |      [Row(min(age)=2)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  alias(self, alias)\n",
      " |      Returns a new :class:`DataFrame` with an alias set.\n",
      " |      \n",
      " |      :param alias: string, an alias name to be set for the :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> from pyspark.sql.functions import *\n",
      " |      >>> df_as1 = df.alias(\"df_as1\")\n",
      " |      >>> df_as2 = df.alias(\"df_as2\")\n",
      " |      >>> joined_df = df_as1.join(df_as2, col(\"df_as1.name\") == col(\"df_as2.name\"), 'inner')\n",
      " |      >>> joined_df.select(\"df_as1.name\", \"df_as2.name\", \"df_as2.age\")                 .sort(desc(\"df_as1.name\")).collect()\n",
      " |      [Row(name='Bob', name='Bob', age=5), Row(name='Alice', name='Alice', age=2)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  approxQuantile(self, col, probabilities, relativeError)\n",
      " |      Calculates the approximate quantiles of numerical columns of a\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      The result of this algorithm has the following deterministic bound:\n",
      " |      If the :class:`DataFrame` has N elements and if we request the quantile at\n",
      " |      probability `p` up to error `err`, then the algorithm will return\n",
      " |      a sample `x` from the :class:`DataFrame` so that the *exact* rank of `x` is\n",
      " |      close to (p * N). More precisely,\n",
      " |      \n",
      " |        floor((p - err) * N) <= rank(x) <= ceil((p + err) * N).\n",
      " |      \n",
      " |      This method implements a variation of the Greenwald-Khanna\n",
      " |      algorithm (with some speed optimizations). The algorithm was first\n",
      " |      present in [[https://doi.org/10.1145/375663.375670\n",
      " |      Space-efficient Online Computation of Quantile Summaries]]\n",
      " |      by Greenwald and Khanna.\n",
      " |      \n",
      " |      Note that null values will be ignored in numerical columns before calculation.\n",
      " |      For columns only containing null values, an empty list is returned.\n",
      " |      \n",
      " |      :param col: str, list.\n",
      " |        Can be a single column name, or a list of names for multiple columns.\n",
      " |      :param probabilities: a list of quantile probabilities\n",
      " |        Each number must belong to [0, 1].\n",
      " |        For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n",
      " |      :param relativeError:  The relative target precision to achieve\n",
      " |        (>= 0). If set to zero, the exact quantiles are computed, which\n",
      " |        could be very expensive. Note that values greater than 1 are\n",
      " |        accepted but give the same result as 1.\n",
      " |      :return:  the approximate quantiles at the given probabilities. If\n",
      " |        the input `col` is a string, the output is a list of floats. If the\n",
      " |        input `col` is a list or tuple of strings, the output is also a\n",
      " |        list, but each element in it is a list of floats, i.e., the output\n",
      " |        is a list of list of floats.\n",
      " |      \n",
      " |      .. versionchanged:: 2.2\n",
      " |         Added support for multiple columns.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  cache(self)\n",
      " |      Persists the :class:`DataFrame` with the default storage level (`MEMORY_AND_DISK`).\n",
      " |      \n",
      " |      .. note:: The default storage level has changed to `MEMORY_AND_DISK` to match Scala in 2.0.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  checkpoint(self, eager=True)\n",
      " |      Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the\n",
      " |      logical plan of this :class:`DataFrame`, which is especially useful in iterative algorithms\n",
      " |      where the plan may grow exponentially. It will be saved to files inside the checkpoint\n",
      " |      directory set with :meth:`SparkContext.setCheckpointDir`.\n",
      " |      \n",
      " |      :param eager: Whether to checkpoint this :class:`DataFrame` immediately\n",
      " |      \n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      .. versionadded:: 2.1\n",
      " |  \n",
      " |  coalesce(self, numPartitions)\n",
      " |      Returns a new :class:`DataFrame` that has exactly `numPartitions` partitions.\n",
      " |      \n",
      " |      :param numPartitions: int, to specify the target number of partitions\n",
      " |      \n",
      " |      Similar to coalesce defined on an :class:`RDD`, this operation results in a\n",
      " |      narrow dependency, e.g. if you go from 1000 partitions to 100 partitions,\n",
      " |      there will not be a shuffle, instead each of the 100 new partitions will\n",
      " |      claim 10 of the current partitions. If a larger number of partitions is requested,\n",
      " |      it will stay at the current number of partitions.\n",
      " |      \n",
      " |      However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,\n",
      " |      this may result in your computation taking place on fewer nodes than\n",
      " |      you like (e.g. one node in the case of numPartitions = 1). To avoid this,\n",
      " |      you can call repartition(). This will add a shuffle step, but means the\n",
      " |      current upstream partitions will be executed in parallel (per whatever\n",
      " |      the current partitioning is).\n",
      " |      \n",
      " |      >>> df.coalesce(1).rdd.getNumPartitions()\n",
      " |      1\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  colRegex(self, colName)\n",
      " |      Selects column based on the column name specified as a regex and returns it\n",
      " |      as :class:`Column`.\n",
      " |      \n",
      " |      :param colName: string, column name specified as a regex.\n",
      " |      \n",
      " |      >>> df = spark.createDataFrame([(\"a\", 1), (\"b\", 2), (\"c\",  3)], [\"Col1\", \"Col2\"])\n",
      " |      >>> df.select(df.colRegex(\"`(Col1)?+.+`\")).show()\n",
      " |      +----+\n",
      " |      |Col2|\n",
      " |      +----+\n",
      " |      |   1|\n",
      " |      |   2|\n",
      " |      |   3|\n",
      " |      +----+\n",
      " |      \n",
      " |      .. versionadded:: 2.3\n",
      " |  \n",
      " |  collect(self)\n",
      " |      Returns all the records as a list of :class:`Row`.\n",
      " |      \n",
      " |      >>> df.collect()\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  corr(self, col1, col2, method=None)\n",
      " |      Calculates the correlation of two columns of a :class:`DataFrame` as a double value.\n",
      " |      Currently only supports the Pearson Correlation Coefficient.\n",
      " |      :func:`DataFrame.corr` and :func:`DataFrameStatFunctions.corr` are aliases of each other.\n",
      " |      \n",
      " |      :param col1: The name of the first column\n",
      " |      :param col2: The name of the second column\n",
      " |      :param method: The correlation method. Currently only supports \"pearson\"\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  count(self)\n",
      " |      Returns the number of rows in this :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> df.count()\n",
      " |      2\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  cov(self, col1, col2)\n",
      " |      Calculate the sample covariance for the given columns, specified by their names, as a\n",
      " |      double value. :func:`DataFrame.cov` and :func:`DataFrameStatFunctions.cov` are aliases.\n",
      " |      \n",
      " |      :param col1: The name of the first column\n",
      " |      :param col2: The name of the second column\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  createGlobalTempView(self, name)\n",
      " |      Creates a global temporary view with this :class:`DataFrame`.\n",
      " |      \n",
      " |      The lifetime of this temporary view is tied to this Spark application.\n",
      " |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
      " |      catalog.\n",
      " |      \n",
      " |      >>> df.createGlobalTempView(\"people\")\n",
      " |      >>> df2 = spark.sql(\"select * from global_temp.people\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> df.createGlobalTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |      ...\n",
      " |      AnalysisException: u\"Temporary table 'people' already exists;\"\n",
      " |      >>> spark.catalog.dropGlobalTempView(\"people\")\n",
      " |      \n",
      " |      .. versionadded:: 2.1\n",
      " |  \n",
      " |  createOrReplaceGlobalTempView(self, name)\n",
      " |      Creates or replaces a global temporary view using the given name.\n",
      " |      \n",
      " |      The lifetime of this temporary view is tied to this Spark application.\n",
      " |      \n",
      " |      >>> df.createOrReplaceGlobalTempView(\"people\")\n",
      " |      >>> df2 = df.filter(df.age > 3)\n",
      " |      >>> df2.createOrReplaceGlobalTempView(\"people\")\n",
      " |      >>> df3 = spark.sql(\"select * from global_temp.people\")\n",
      " |      >>> sorted(df3.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> spark.catalog.dropGlobalTempView(\"people\")\n",
      " |      \n",
      " |      .. versionadded:: 2.2\n",
      " |  \n",
      " |  createOrReplaceTempView(self, name)\n",
      " |      Creates or replaces a local temporary view with this :class:`DataFrame`.\n",
      " |      \n",
      " |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      " |      that was used to create this :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> df.createOrReplaceTempView(\"people\")\n",
      " |      >>> df2 = df.filter(df.age > 3)\n",
      " |      >>> df2.createOrReplaceTempView(\"people\")\n",
      " |      >>> df3 = spark.sql(\"select * from people\")\n",
      " |      >>> sorted(df3.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> spark.catalog.dropTempView(\"people\")\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  createTempView(self, name)\n",
      " |      Creates a local temporary view with this :class:`DataFrame`.\n",
      " |      \n",
      " |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      " |      that was used to create this :class:`DataFrame`.\n",
      " |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
      " |      catalog.\n",
      " |      \n",
      " |      >>> df.createTempView(\"people\")\n",
      " |      >>> df2 = spark.sql(\"select * from people\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> df.createTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |      ...\n",
      " |      AnalysisException: u\"Temporary table 'people' already exists;\"\n",
      " |      >>> spark.catalog.dropTempView(\"people\")\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  crossJoin(self, other)\n",
      " |      Returns the cartesian product with another :class:`DataFrame`.\n",
      " |      \n",
      " |      :param other: Right side of the cartesian product.\n",
      " |      \n",
      " |      >>> df.select(\"age\", \"name\").collect()\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      >>> df2.select(\"name\", \"height\").collect()\n",
      " |      [Row(name='Tom', height=80), Row(name='Bob', height=85)]\n",
      " |      >>> df.crossJoin(df2.select(\"height\")).select(\"age\", \"name\", \"height\").collect()\n",
      " |      [Row(age=2, name='Alice', height=80), Row(age=2, name='Alice', height=85),\n",
      " |       Row(age=5, name='Bob', height=80), Row(age=5, name='Bob', height=85)]\n",
      " |      \n",
      " |      .. versionadded:: 2.1\n",
      " |  \n",
      " |  crosstab(self, col1, col2)\n",
      " |      Computes a pair-wise frequency table of the given columns. Also known as a contingency\n",
      " |      table. The number of distinct values for each column should be less than 1e4. At most 1e6\n",
      " |      non-zero pair frequencies will be returned.\n",
      " |      The first column of each row will be the distinct values of `col1` and the column names\n",
      " |      will be the distinct values of `col2`. The name of the first column will be `$col1_$col2`.\n",
      " |      Pairs that have no occurrences will have zero as their counts.\n",
      " |      :func:`DataFrame.crosstab` and :func:`DataFrameStatFunctions.crosstab` are aliases.\n",
      " |      \n",
      " |      :param col1: The name of the first column. Distinct items will make the first item of\n",
      " |          each row.\n",
      " |      :param col2: The name of the second column. Distinct items will make the column names\n",
      " |          of the :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  cube(self, *cols)\n",
      " |      Create a multi-dimensional cube for the current :class:`DataFrame` using\n",
      " |      the specified columns, so we can run aggregations on them.\n",
      " |      \n",
      " |      >>> df.cube(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
      " |      +-----+----+-----+\n",
      " |      | name| age|count|\n",
      " |      +-----+----+-----+\n",
      " |      | null|null|    2|\n",
      " |      | null|   2|    1|\n",
      " |      | null|   5|    1|\n",
      " |      |Alice|null|    1|\n",
      " |      |Alice|   2|    1|\n",
      " |      |  Bob|null|    1|\n",
      " |      |  Bob|   5|    1|\n",
      " |      +-----+----+-----+\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  describe(self, *cols)\n",
      " |      Computes basic statistics for numeric and string columns.\n",
      " |      \n",
      " |      This include count, mean, stddev, min, and max. If no columns are\n",
      " |      given, this function computes statistics for all numerical or string columns.\n",
      " |      \n",
      " |      .. note:: This function is meant for exploratory data analysis, as we make no\n",
      " |          guarantee about the backward compatibility of the schema of the resulting\n",
      " |          :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> df.describe(['age']).show()\n",
      " |      +-------+------------------+\n",
      " |      |summary|               age|\n",
      " |      +-------+------------------+\n",
      " |      |  count|                 2|\n",
      " |      |   mean|               3.5|\n",
      " |      | stddev|2.1213203435596424|\n",
      " |      |    min|                 2|\n",
      " |      |    max|                 5|\n",
      " |      +-------+------------------+\n",
      " |      >>> df.describe().show()\n",
      " |      +-------+------------------+-----+\n",
      " |      |summary|               age| name|\n",
      " |      +-------+------------------+-----+\n",
      " |      |  count|                 2|    2|\n",
      " |      |   mean|               3.5| null|\n",
      " |      | stddev|2.1213203435596424| null|\n",
      " |      |    min|                 2|Alice|\n",
      " |      |    max|                 5|  Bob|\n",
      " |      +-------+------------------+-----+\n",
      " |      \n",
      " |      Use summary for expanded statistics and control over which statistics to compute.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |  \n",
      " |  distinct(self)\n",
      " |      Returns a new :class:`DataFrame` containing the distinct rows in this :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> df.distinct().count()\n",
      " |      2\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  drop(self, *cols)\n",
      " |      Returns a new :class:`DataFrame` that drops the specified column.\n",
      " |      This is a no-op if schema doesn't contain the given column name(s).\n",
      " |      \n",
      " |      :param cols: a string name of the column to drop, or a\n",
      " |          :class:`Column` to drop, or a list of string name of the columns to drop.\n",
      " |      \n",
      " |      >>> df.drop('age').collect()\n",
      " |      [Row(name='Alice'), Row(name='Bob')]\n",
      " |      \n",
      " |      >>> df.drop(df.age).collect()\n",
      " |      [Row(name='Alice'), Row(name='Bob')]\n",
      " |      \n",
      " |      >>> df.join(df2, df.name == df2.name, 'inner').drop(df.name).collect()\n",
      " |      [Row(age=5, height=85, name='Bob')]\n",
      " |      \n",
      " |      >>> df.join(df2, df.name == df2.name, 'inner').drop(df2.name).collect()\n",
      " |      [Row(age=5, name='Bob', height=85)]\n",
      " |      \n",
      " |      >>> df.join(df2, 'name', 'inner').drop('age', 'height').collect()\n",
      " |      [Row(name='Bob')]\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  dropDuplicates(self, subset=None)\n",
      " |      Return a new :class:`DataFrame` with duplicate rows removed,\n",
      " |      optionally only considering certain columns.\n",
      " |      \n",
      " |      For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n",
      " |      :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n",
      " |      duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n",
      " |      be and system will accordingly limit the state. In addition, too late data older than\n",
      " |      watermark will be dropped to avoid any possibility of duplicates.\n",
      " |      \n",
      " |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = sc.parallelize([ \\\n",
      " |      ...     Row(name='Alice', age=5, height=80), \\\n",
      " |      ...     Row(name='Alice', age=5, height=80), \\\n",
      " |      ...     Row(name='Alice', age=10, height=80)]).toDF()\n",
      " |      >>> df.dropDuplicates().show()\n",
      " |      +---+------+-----+\n",
      " |      |age|height| name|\n",
      " |      +---+------+-----+\n",
      " |      |  5|    80|Alice|\n",
      " |      | 10|    80|Alice|\n",
      " |      +---+------+-----+\n",
      " |      \n",
      " |      >>> df.dropDuplicates(['name', 'height']).show()\n",
      " |      +---+------+-----+\n",
      " |      |age|height| name|\n",
      " |      +---+------+-----+\n",
      " |      |  5|    80|Alice|\n",
      " |      +---+------+-----+\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  drop_duplicates = dropDuplicates(self, subset=None)\n",
      " |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  dropna(self, how='any', thresh=None, subset=None)\n",
      " |      Returns a new :class:`DataFrame` omitting rows with null values.\n",
      " |      :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n",
      " |      \n",
      " |      :param how: 'any' or 'all'.\n",
      " |          If 'any', drop a row if it contains any nulls.\n",
      " |          If 'all', drop a row only if all its values are null.\n",
      " |      :param thresh: int, default None\n",
      " |          If specified, drop rows that have less than `thresh` non-null values.\n",
      " |          This overwrites the `how` parameter.\n",
      " |      :param subset: optional list of column names to consider.\n",
      " |      \n",
      " |      >>> df4.na.drop().show()\n",
      " |      +---+------+-----+\n",
      " |      |age|height| name|\n",
      " |      +---+------+-----+\n",
      " |      | 10|    80|Alice|\n",
      " |      +---+------+-----+\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |  \n",
      " |  exceptAll(self, other)\n",
      " |      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame` but\n",
      " |      not in another :class:`DataFrame` while preserving duplicates.\n",
      " |      \n",
      " |      This is equivalent to `EXCEPT ALL` in SQL.\n",
      " |      \n",
      " |      >>> df1 = spark.createDataFrame(\n",
      " |      ...         [(\"a\", 1), (\"a\", 1), (\"a\", 1), (\"a\", 2), (\"b\",  3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      " |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      " |      \n",
      " |      >>> df1.exceptAll(df2).show()\n",
      " |      +---+---+\n",
      " |      | C1| C2|\n",
      " |      +---+---+\n",
      " |      |  a|  1|\n",
      " |      |  a|  1|\n",
      " |      |  a|  2|\n",
      " |      |  c|  4|\n",
      " |      +---+---+\n",
      " |      \n",
      " |      Also as standard in SQL, this function resolves columns by position (not by name).\n",
      " |      \n",
      " |      .. versionadded:: 2.4\n",
      " |  \n",
      " |  explain(self, extended=None, mode=None)\n",
      " |      Prints the (logical and physical) plans to the console for debugging purpose.\n",
      " |      \n",
      " |      :param extended: boolean, default ``False``. If ``False``, prints only the physical plan.\n",
      " |          When this is a string without specifying the ``mode``, it works as the mode is\n",
      " |          specified.\n",
      " |      :param mode: specifies the expected output format of plans.\n",
      " |      \n",
      " |          * ``simple``: Print only a physical plan.\n",
      " |          * ``extended``: Print both logical and physical plans.\n",
      " |          * ``codegen``: Print a physical plan and generated codes if they are available.\n",
      " |          * ``cost``: Print a logical plan and statistics if they are available.\n",
      " |          * ``formatted``: Split explain output into two sections: a physical plan outline                 and node details.\n",
      " |      \n",
      " |      >>> df.explain()\n",
      " |      == Physical Plan ==\n",
      " |      *(1) Scan ExistingRDD[age#0,name#1]\n",
      " |      \n",
      " |      >>> df.explain(True)\n",
      " |      == Parsed Logical Plan ==\n",
      " |      ...\n",
      " |      == Analyzed Logical Plan ==\n",
      " |      ...\n",
      " |      == Optimized Logical Plan ==\n",
      " |      ...\n",
      " |      == Physical Plan ==\n",
      " |      ...\n",
      " |      \n",
      " |      >>> df.explain(mode=\"formatted\")\n",
      " |      == Physical Plan ==\n",
      " |      * Scan ExistingRDD (1)\n",
      " |      (1) Scan ExistingRDD [codegen id : 1]\n",
      " |      Output [2]: [age#0, name#1]\n",
      " |      ...\n",
      " |      \n",
      " |      >>> df.explain(\"cost\")\n",
      " |      == Optimized Logical Plan ==\n",
      " |      ...Statistics...\n",
      " |      ...\n",
      " |      \n",
      " |      .. versionchanged:: 3.0.0\n",
      " |         Added optional argument `mode` to specify the expected output format of plans.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  fillna(self, value, subset=None)\n",
      " |      Replace null values, alias for ``na.fill()``.\n",
      " |      :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n",
      " |      \n",
      " |      :param value: int, long, float, string, bool or dict.\n",
      " |          Value to replace null values with.\n",
      " |          If the value is a dict, then `subset` is ignored and `value` must be a mapping\n",
      " |          from column name (string) to replacement value. The replacement value must be\n",
      " |          an int, long, float, boolean, or string.\n",
      " |      :param subset: optional list of column names to consider.\n",
      " |          Columns specified in subset that do not have matching data type are ignored.\n",
      " |          For example, if `value` is a string, and subset contains a non-string column,\n",
      " |          then the non-string column is simply ignored.\n",
      " |      \n",
      " |      >>> df4.na.fill(50).show()\n",
      " |      +---+------+-----+\n",
      " |      |age|height| name|\n",
      " |      +---+------+-----+\n",
      " |      | 10|    80|Alice|\n",
      " |      |  5|    50|  Bob|\n",
      " |      | 50|    50|  Tom|\n",
      " |      | 50|    50| null|\n",
      " |      +---+------+-----+\n",
      " |      \n",
      " |      >>> df5.na.fill(False).show()\n",
      " |      +----+-------+-----+\n",
      " |      | age|   name|  spy|\n",
      " |      +----+-------+-----+\n",
      " |      |  10|  Alice|false|\n",
      " |      |   5|    Bob|false|\n",
      " |      |null|Mallory| true|\n",
      " |      +----+-------+-----+\n",
      " |      \n",
      " |      >>> df4.na.fill({'age': 50, 'name': 'unknown'}).show()\n",
      " |      +---+------+-------+\n",
      " |      |age|height|   name|\n",
      " |      +---+------+-------+\n",
      " |      | 10|    80|  Alice|\n",
      " |      |  5|  null|    Bob|\n",
      " |      | 50|  null|    Tom|\n",
      " |      | 50|  null|unknown|\n",
      " |      +---+------+-------+\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |  \n",
      " |  filter(self, condition)\n",
      " |      Filters rows using the given condition.\n",
      " |      \n",
      " |      :func:`where` is an alias for :func:`filter`.\n",
      " |      \n",
      " |      :param condition: a :class:`Column` of :class:`types.BooleanType`\n",
      " |          or a string of SQL expression.\n",
      " |      \n",
      " |      >>> df.filter(df.age > 3).collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      >>> df.where(df.age == 2).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      \n",
      " |      >>> df.filter(\"age > 3\").collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      >>> df.where(\"age = 2\").collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  first(self)\n",
      " |      Returns the first row as a :class:`Row`.\n",
      " |      \n",
      " |      >>> df.first()\n",
      " |      Row(age=2, name='Alice')\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  foreach(self, f)\n",
      " |      Applies the ``f`` function to all :class:`Row` of this :class:`DataFrame`.\n",
      " |      \n",
      " |      This is a shorthand for ``df.rdd.foreach()``.\n",
      " |      \n",
      " |      >>> def f(person):\n",
      " |      ...     print(person.name)\n",
      " |      >>> df.foreach(f)\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  foreachPartition(self, f)\n",
      " |      Applies the ``f`` function to each partition of this :class:`DataFrame`.\n",
      " |      \n",
      " |      This a shorthand for ``df.rdd.foreachPartition()``.\n",
      " |      \n",
      " |      >>> def f(people):\n",
      " |      ...     for person in people:\n",
      " |      ...         print(person.name)\n",
      " |      >>> df.foreachPartition(f)\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  freqItems(self, cols, support=None)\n",
      " |      Finding frequent items for columns, possibly with false positives. Using the\n",
      " |      frequent element count algorithm described in\n",
      " |      \"https://doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou\".\n",
      " |      :func:`DataFrame.freqItems` and :func:`DataFrameStatFunctions.freqItems` are aliases.\n",
      " |      \n",
      " |      .. note:: This function is meant for exploratory data analysis, as we make no\n",
      " |          guarantee about the backward compatibility of the schema of the resulting\n",
      " |          :class:`DataFrame`.\n",
      " |      \n",
      " |      :param cols: Names of the columns to calculate frequent items for as a list or tuple of\n",
      " |          strings.\n",
      " |      :param support: The frequency with which to consider an item 'frequent'. Default is 1%.\n",
      " |          The support must be greater than 1e-4.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  groupBy(self, *cols)\n",
      " |      Groups the :class:`DataFrame` using the specified columns,\n",
      " |      so we can run aggregation on them. See :class:`GroupedData`\n",
      " |      for all the available aggregate functions.\n",
      " |      \n",
      " |      :func:`groupby` is an alias for :func:`groupBy`.\n",
      " |      \n",
      " |      :param cols: list of columns to group by.\n",
      " |          Each element should be a column name (string) or an expression (:class:`Column`).\n",
      " |      \n",
      " |      >>> df.groupBy().avg().collect()\n",
      " |      [Row(avg(age)=3.5)]\n",
      " |      >>> sorted(df.groupBy('name').agg({'age': 'mean'}).collect())\n",
      " |      [Row(name='Alice', avg(age)=2.0), Row(name='Bob', avg(age)=5.0)]\n",
      " |      >>> sorted(df.groupBy(df.name).avg().collect())\n",
      " |      [Row(name='Alice', avg(age)=2.0), Row(name='Bob', avg(age)=5.0)]\n",
      " |      >>> sorted(df.groupBy(['name', df.age]).count().collect())\n",
      " |      [Row(name='Alice', age=2, count=1), Row(name='Bob', age=5, count=1)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  groupby = groupBy(self, *cols)\n",
      " |      :func:`groupby` is an alias for :func:`groupBy`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  head(self, n=None)\n",
      " |      Returns the first ``n`` rows.\n",
      " |      \n",
      " |      .. note:: This method should only be used if the resulting array is expected\n",
      " |          to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      :param n: int, default 1. Number of rows to return.\n",
      " |      :return: If n is greater than 1, return a list of :class:`Row`.\n",
      " |          If n is 1, return a single Row.\n",
      " |      \n",
      " |      >>> df.head()\n",
      " |      Row(age=2, name='Alice')\n",
      " |      >>> df.head(1)\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  hint(self, name, *parameters)\n",
      " |      Specifies some hint on the current :class:`DataFrame`.\n",
      " |      \n",
      " |      :param name: A name of the hint.\n",
      " |      :param parameters: Optional parameters.\n",
      " |      :return: :class:`DataFrame`\n",
      " |      \n",
      " |      >>> df.join(df2.hint(\"broadcast\"), \"name\").show()\n",
      " |      +----+---+------+\n",
      " |      |name|age|height|\n",
      " |      +----+---+------+\n",
      " |      | Bob|  5|    85|\n",
      " |      +----+---+------+\n",
      " |      \n",
      " |      .. versionadded:: 2.2\n",
      " |  \n",
      " |  intersect(self, other)\n",
      " |      Return a new :class:`DataFrame` containing rows only in\n",
      " |      both this :class:`DataFrame` and another :class:`DataFrame`.\n",
      " |      \n",
      " |      This is equivalent to `INTERSECT` in SQL.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  intersectAll(self, other)\n",
      " |      Return a new :class:`DataFrame` containing rows in both this :class:`DataFrame`\n",
      " |      and another :class:`DataFrame` while preserving duplicates.\n",
      " |      \n",
      " |      This is equivalent to `INTERSECT ALL` in SQL.\n",
      " |      >>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      " |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      " |      \n",
      " |      >>> df1.intersectAll(df2).sort(\"C1\", \"C2\").show()\n",
      " |      +---+---+\n",
      " |      | C1| C2|\n",
      " |      +---+---+\n",
      " |      |  a|  1|\n",
      " |      |  a|  1|\n",
      " |      |  b|  3|\n",
      " |      +---+---+\n",
      " |      \n",
      " |      Also as standard in SQL, this function resolves columns by position (not by name).\n",
      " |      \n",
      " |      .. versionadded:: 2.4\n",
      " |  \n",
      " |  isLocal(self)\n",
      " |      Returns ``True`` if the :func:`collect` and :func:`take` methods can be run locally\n",
      " |      (without any Spark executors).\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  join(self, other, on=None, how=None)\n",
      " |      Joins with another :class:`DataFrame`, using the given join expression.\n",
      " |      \n",
      " |      :param other: Right side of the join\n",
      " |      :param on: a string for the join column name, a list of column names,\n",
      " |          a join expression (Column), or a list of Columns.\n",
      " |          If `on` is a string or a list of strings indicating the name of the join column(s),\n",
      " |          the column(s) must exist on both sides, and this performs an equi-join.\n",
      " |      :param how: str, default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,\n",
      " |          ``full``, ``fullouter``, ``full_outer``, ``left``, ``leftouter``, ``left_outer``,\n",
      " |          ``right``, ``rightouter``, ``right_outer``, ``semi``, ``leftsemi``, ``left_semi``,\n",
      " |          ``anti``, ``leftanti`` and ``left_anti``.\n",
      " |      \n",
      " |      The following performs a full outer join between ``df1`` and ``df2``.\n",
      " |      >>> from pyspark.sql.functions import desc\n",
      " |      >>> df.join(df2, df.name == df2.name, 'outer').select(df.name, df2.height)                 .sort(desc(\"name\")).collect()\n",
      " |      [Row(name='Bob', height=85), Row(name='Alice', height=None), Row(name=None, height=80)]\n",
      " |      \n",
      " |      >>> df.join(df2, 'name', 'outer').select('name', 'height').sort(desc(\"name\")).collect()\n",
      " |      [Row(name='Tom', height=80), Row(name='Bob', height=85), Row(name='Alice', height=None)]\n",
      " |      \n",
      " |      >>> cond = [df.name == df3.name, df.age == df3.age]\n",
      " |      >>> df.join(df3, cond, 'outer').select(df.name, df3.age).collect()\n",
      " |      [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      " |      \n",
      " |      >>> df.join(df2, 'name').select(df.name, df2.height).collect()\n",
      " |      [Row(name='Bob', height=85)]\n",
      " |      \n",
      " |      >>> df.join(df4, ['name', 'age']).select(df.name, df.age).collect()\n",
      " |      [Row(name='Bob', age=5)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  limit(self, num)\n",
      " |      Limits the result count to the number specified.\n",
      " |      \n",
      " |      >>> df.limit(1).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      >>> df.limit(0).collect()\n",
      " |      []\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  localCheckpoint(self, eager=True)\n",
      " |      Returns a locally checkpointed version of this Dataset. Checkpointing can be used to\n",
      " |      truncate the logical plan of this :class:`DataFrame`, which is especially useful in\n",
      " |      iterative algorithms where the plan may grow exponentially. Local checkpoints are\n",
      " |      stored in the executors using the caching subsystem and therefore they are not reliable.\n",
      " |      \n",
      " |      :param eager: Whether to checkpoint this :class:`DataFrame` immediately\n",
      " |      \n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      .. versionadded:: 2.3\n",
      " |  \n",
      " |  orderBy = sort(self, *cols, **kwargs)\n",
      " |  \n",
      " |  persist(self, storageLevel=StorageLevel(True, True, False, False, 1))\n",
      " |      Sets the storage level to persist the contents of the :class:`DataFrame` across\n",
      " |      operations after the first time it is computed. This can only be used to assign\n",
      " |      a new storage level if the :class:`DataFrame` does not have a storage level set yet.\n",
      " |      If no storage level is specified defaults to (`MEMORY_AND_DISK`).\n",
      " |      \n",
      " |      .. note:: The default storage level has changed to `MEMORY_AND_DISK` to match Scala in 2.0.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  printSchema(self)\n",
      " |      Prints out the schema in the tree format.\n",
      " |      \n",
      " |      >>> df.printSchema()\n",
      " |      root\n",
      " |       |-- age: integer (nullable = true)\n",
      " |       |-- name: string (nullable = true)\n",
      " |      <BLANKLINE>\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  randomSplit(self, weights, seed=None)\n",
      " |      Randomly splits this :class:`DataFrame` with the provided weights.\n",
      " |      \n",
      " |      :param weights: list of doubles as weights with which to split the :class:`DataFrame`.\n",
      " |          Weights will be normalized if they don't sum up to 1.0.\n",
      " |      :param seed: The seed for sampling.\n",
      " |      \n",
      " |      >>> splits = df4.randomSplit([1.0, 2.0], 24)\n",
      " |      >>> splits[0].count()\n",
      " |      2\n",
      " |      \n",
      " |      >>> splits[1].count()\n",
      " |      2\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  registerTempTable(self, name)\n",
      " |      Registers this DataFrame as a temporary table using the given name.\n",
      " |      \n",
      " |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      " |      that was used to create this :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> df.registerTempTable(\"people\")\n",
      " |      >>> df2 = spark.sql(\"select * from people\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> spark.catalog.dropTempView(\"people\")\n",
      " |      \n",
      " |      .. note:: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  repartition(self, numPartitions, *cols)\n",
      " |      Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
      " |      resulting :class:`DataFrame` is hash partitioned.\n",
      " |      \n",
      " |      :param numPartitions:\n",
      " |          can be an int to specify the target number of partitions or a Column.\n",
      " |          If it is a Column, it will be used as the first partitioning column. If not specified,\n",
      " |          the default number of partitions is used.\n",
      " |      \n",
      " |      .. versionchanged:: 1.6\n",
      " |         Added optional arguments to specify the partitioning columns. Also made numPartitions\n",
      " |         optional if partitioning columns are specified.\n",
      " |      \n",
      " |      >>> df.repartition(10).rdd.getNumPartitions()\n",
      " |      10\n",
      " |      >>> data = df.union(df).repartition(\"age\")\n",
      " |      >>> data.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  5|  Bob|\n",
      " |      |  5|  Bob|\n",
      " |      |  2|Alice|\n",
      " |      |  2|Alice|\n",
      " |      +---+-----+\n",
      " |      >>> data = data.repartition(7, \"age\")\n",
      " |      >>> data.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      >>> data.rdd.getNumPartitions()\n",
      " |      7\n",
      " |      >>> data = data.repartition(\"name\", \"age\")\n",
      " |      >>> data.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  5|  Bob|\n",
      " |      |  5|  Bob|\n",
      " |      |  2|Alice|\n",
      " |      |  2|Alice|\n",
      " |      +---+-----+\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  repartitionByRange(self, numPartitions, *cols)\n",
      " |      Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
      " |      resulting :class:`DataFrame` is range partitioned.\n",
      " |      \n",
      " |      :param numPartitions:\n",
      " |          can be an int to specify the target number of partitions or a Column.\n",
      " |          If it is a Column, it will be used as the first partitioning column. If not specified,\n",
      " |          the default number of partitions is used.\n",
      " |      \n",
      " |      At least one partition-by expression must be specified.\n",
      " |      When no explicit sort order is specified, \"ascending nulls first\" is assumed.\n",
      " |      \n",
      " |      Note that due to performance reasons this method uses sampling to estimate the ranges.\n",
      " |      Hence, the output may not be consistent, since sampling can return different values.\n",
      " |      The sample size can be controlled by the config\n",
      " |      `spark.sql.execution.rangeExchange.sampleSizePerPartition`.\n",
      " |      \n",
      " |      >>> df.repartitionByRange(2, \"age\").rdd.getNumPartitions()\n",
      " |      2\n",
      " |      >>> df.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      >>> df.repartitionByRange(1, \"age\").rdd.getNumPartitions()\n",
      " |      1\n",
      " |      >>> data = df.repartitionByRange(\"age\")\n",
      " |      >>> df.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |  \n",
      " |  replace(self, to_replace, value=<no value>, subset=None)\n",
      " |      Returns a new :class:`DataFrame` replacing a value with another value.\n",
      " |      :func:`DataFrame.replace` and :func:`DataFrameNaFunctions.replace` are\n",
      " |      aliases of each other.\n",
      " |      Values to_replace and value must have the same type and can only be numerics, booleans,\n",
      " |      or strings. Value can have None. When replacing, the new value will be cast\n",
      " |      to the type of the existing column.\n",
      " |      For numeric replacements all values to be replaced should have unique\n",
      " |      floating point representation. In case of conflicts (for example with `{42: -1, 42.0: 1}`)\n",
      " |      and arbitrary replacement will be used.\n",
      " |      \n",
      " |      :param to_replace: bool, int, long, float, string, list or dict.\n",
      " |          Value to be replaced.\n",
      " |          If the value is a dict, then `value` is ignored or can be omitted, and `to_replace`\n",
      " |          must be a mapping between a value and a replacement.\n",
      " |      :param value: bool, int, long, float, string, list or None.\n",
      " |          The replacement value must be a bool, int, long, float, string or None. If `value` is a\n",
      " |          list, `value` should be of the same length and type as `to_replace`.\n",
      " |          If `value` is a scalar and `to_replace` is a sequence, then `value` is\n",
      " |          used as a replacement for each item in `to_replace`.\n",
      " |      :param subset: optional list of column names to consider.\n",
      " |          Columns specified in subset that do not have matching data type are ignored.\n",
      " |          For example, if `value` is a string, and subset contains a non-string column,\n",
      " |          then the non-string column is simply ignored.\n",
      " |      \n",
      " |      >>> df4.na.replace(10, 20).show()\n",
      " |      +----+------+-----+\n",
      " |      | age|height| name|\n",
      " |      +----+------+-----+\n",
      " |      |  20|    80|Alice|\n",
      " |      |   5|  null|  Bob|\n",
      " |      |null|  null|  Tom|\n",
      " |      |null|  null| null|\n",
      " |      +----+------+-----+\n",
      " |      \n",
      " |      >>> df4.na.replace('Alice', None).show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|null|\n",
      " |      |   5|  null| Bob|\n",
      " |      |null|  null| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |      \n",
      " |      >>> df4.na.replace({'Alice': None}).show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|null|\n",
      " |      |   5|  null| Bob|\n",
      " |      |null|  null| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |      \n",
      " |      >>> df4.na.replace(['Alice', 'Bob'], ['A', 'B'], 'name').show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|   A|\n",
      " |      |   5|  null|   B|\n",
      " |      |null|  null| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  rollup(self, *cols)\n",
      " |      Create a multi-dimensional rollup for the current :class:`DataFrame` using\n",
      " |      the specified columns, so we can run aggregation on them.\n",
      " |      \n",
      " |      >>> df.rollup(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
      " |      +-----+----+-----+\n",
      " |      | name| age|count|\n",
      " |      +-----+----+-----+\n",
      " |      | null|null|    2|\n",
      " |      |Alice|null|    1|\n",
      " |      |Alice|   2|    1|\n",
      " |      |  Bob|null|    1|\n",
      " |      |  Bob|   5|    1|\n",
      " |      +-----+----+-----+\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  sample(self, withReplacement=None, fraction=None, seed=None)\n",
      " |      Returns a sampled subset of this :class:`DataFrame`.\n",
      " |      \n",
      " |      :param withReplacement: Sample with replacement or not (default ``False``).\n",
      " |      :param fraction: Fraction of rows to generate, range [0.0, 1.0].\n",
      " |      :param seed: Seed for sampling (default a random seed).\n",
      " |      \n",
      " |      .. note:: This is not guaranteed to provide exactly the fraction specified of the total\n",
      " |          count of the given :class:`DataFrame`.\n",
      " |      \n",
      " |      .. note:: `fraction` is required and, `withReplacement` and `seed` are optional.\n",
      " |      \n",
      " |      >>> df = spark.range(10)\n",
      " |      >>> df.sample(0.5, 3).count()\n",
      " |      7\n",
      " |      >>> df.sample(fraction=0.5, seed=3).count()\n",
      " |      7\n",
      " |      >>> df.sample(withReplacement=True, fraction=0.5, seed=3).count()\n",
      " |      1\n",
      " |      >>> df.sample(1.0).count()\n",
      " |      10\n",
      " |      >>> df.sample(fraction=1.0).count()\n",
      " |      10\n",
      " |      >>> df.sample(False, fraction=1.0).count()\n",
      " |      10\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  sampleBy(self, col, fractions, seed=None)\n",
      " |      Returns a stratified sample without replacement based on the\n",
      " |      fraction given on each stratum.\n",
      " |      \n",
      " |      :param col: column that defines strata\n",
      " |      :param fractions:\n",
      " |          sampling fraction for each stratum. If a stratum is not\n",
      " |          specified, we treat its fraction as zero.\n",
      " |      :param seed: random seed\n",
      " |      :return: a new :class:`DataFrame` that represents the stratified sample\n",
      " |      \n",
      " |      >>> from pyspark.sql.functions import col\n",
      " |      >>> dataset = sqlContext.range(0, 100).select((col(\"id\") % 3).alias(\"key\"))\n",
      " |      >>> sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n",
      " |      >>> sampled.groupBy(\"key\").count().orderBy(\"key\").show()\n",
      " |      +---+-----+\n",
      " |      |key|count|\n",
      " |      +---+-----+\n",
      " |      |  0|    3|\n",
      " |      |  1|    6|\n",
      " |      +---+-----+\n",
      " |      >>> dataset.sampleBy(col(\"key\"), fractions={2: 1.0}, seed=0).count()\n",
      " |      33\n",
      " |      \n",
      " |      .. versionchanged:: 3.0\n",
      " |         Added sampling by a column of :class:`Column`\n",
      " |      \n",
      " |      .. versionadded:: 1.5\n",
      " |  \n",
      " |  select(self, *cols)\n",
      " |      Projects a set of expressions and returns a new :class:`DataFrame`.\n",
      " |      \n",
      " |      :param cols: list of column names (string) or expressions (:class:`Column`).\n",
      " |          If one of the column names is '*', that column is expanded to include all columns\n",
      " |          in the current :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> df.select('*').collect()\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      >>> df.select('name', 'age').collect()\n",
      " |      [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      " |      >>> df.select(df.name, (df.age + 10).alias('age')).collect()\n",
      " |      [Row(name='Alice', age=12), Row(name='Bob', age=15)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  selectExpr(self, *expr)\n",
      " |      Projects a set of SQL expressions and returns a new :class:`DataFrame`.\n",
      " |      \n",
      " |      This is a variant of :func:`select` that accepts SQL expressions.\n",
      " |      \n",
      " |      >>> df.selectExpr(\"age * 2\", \"abs(age)\").collect()\n",
      " |      [Row((age * 2)=4, abs(age)=2), Row((age * 2)=10, abs(age)=5)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  show(self, n=20, truncate=True, vertical=False)\n",
      " |      Prints the first ``n`` rows to the console.\n",
      " |      \n",
      " |      :param n: Number of rows to show.\n",
      " |      :param truncate: If set to ``True``, truncate strings longer than 20 chars by default.\n",
      " |          If set to a number greater than one, truncates long strings to length ``truncate``\n",
      " |          and align cells right.\n",
      " |      :param vertical: If set to ``True``, print output rows vertically (one line\n",
      " |          per column value).\n",
      " |      \n",
      " |      >>> df\n",
      " |      DataFrame[age: int, name: string]\n",
      " |      >>> df.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      >>> df.show(truncate=3)\n",
      " |      +---+----+\n",
      " |      |age|name|\n",
      " |      +---+----+\n",
      " |      |  2| Ali|\n",
      " |      |  5| Bob|\n",
      " |      +---+----+\n",
      " |      >>> df.show(vertical=True)\n",
      " |      -RECORD 0-----\n",
      " |       age  | 2\n",
      " |       name | Alice\n",
      " |      -RECORD 1-----\n",
      " |       age  | 5\n",
      " |       name | Bob\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  sort(self, *cols, **kwargs)\n",
      " |      Returns a new :class:`DataFrame` sorted by the specified column(s).\n",
      " |      \n",
      " |      :param cols: list of :class:`Column` or column names to sort by.\n",
      " |      :param ascending: boolean or list of boolean (default ``True``).\n",
      " |          Sort ascending vs. descending. Specify list for multiple sort orders.\n",
      " |          If a list is specified, length of the list must equal length of the `cols`.\n",
      " |      \n",
      " |      >>> df.sort(df.age.desc()).collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      >>> df.sort(\"age\", ascending=False).collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      >>> df.orderBy(df.age.desc()).collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      >>> from pyspark.sql.functions import *\n",
      " |      >>> df.sort(asc(\"age\")).collect()\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      >>> df.orderBy(desc(\"age\"), \"name\").collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      >>> df.orderBy([\"age\", \"name\"], ascending=[0, 1]).collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  sortWithinPartitions(self, *cols, **kwargs)\n",
      " |      Returns a new :class:`DataFrame` with each partition sorted by the specified column(s).\n",
      " |      \n",
      " |      :param cols: list of :class:`Column` or column names to sort by.\n",
      " |      :param ascending: boolean or list of boolean (default ``True``).\n",
      " |          Sort ascending vs. descending. Specify list for multiple sort orders.\n",
      " |          If a list is specified, length of the list must equal length of the `cols`.\n",
      " |      \n",
      " |      >>> df.sortWithinPartitions(\"age\", ascending=False).show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      \n",
      " |      .. versionadded:: 1.6\n",
      " |  \n",
      " |  subtract(self, other)\n",
      " |      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame`\n",
      " |      but not in another :class:`DataFrame`.\n",
      " |      \n",
      " |      This is equivalent to `EXCEPT DISTINCT` in SQL.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  summary(self, *statistics)\n",
      " |      Computes specified statistics for numeric and string columns. Available statistics are:\n",
      " |      - count\n",
      " |      - mean\n",
      " |      - stddev\n",
      " |      - min\n",
      " |      - max\n",
      " |      - arbitrary approximate percentiles specified as a percentage (eg, 75%)\n",
      " |      \n",
      " |      If no statistics are given, this function computes count, mean, stddev, min,\n",
      " |      approximate quartiles (percentiles at 25%, 50%, and 75%), and max.\n",
      " |      \n",
      " |      .. note:: This function is meant for exploratory data analysis, as we make no\n",
      " |          guarantee about the backward compatibility of the schema of the resulting\n",
      " |          :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> df.summary().show()\n",
      " |      +-------+------------------+-----+\n",
      " |      |summary|               age| name|\n",
      " |      +-------+------------------+-----+\n",
      " |      |  count|                 2|    2|\n",
      " |      |   mean|               3.5| null|\n",
      " |      | stddev|2.1213203435596424| null|\n",
      " |      |    min|                 2|Alice|\n",
      " |      |    25%|                 2| null|\n",
      " |      |    50%|                 2| null|\n",
      " |      |    75%|                 5| null|\n",
      " |      |    max|                 5|  Bob|\n",
      " |      +-------+------------------+-----+\n",
      " |      \n",
      " |      >>> df.summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show()\n",
      " |      +-------+---+-----+\n",
      " |      |summary|age| name|\n",
      " |      +-------+---+-----+\n",
      " |      |  count|  2|    2|\n",
      " |      |    min|  2|Alice|\n",
      " |      |    25%|  2| null|\n",
      " |      |    75%|  5| null|\n",
      " |      |    max|  5|  Bob|\n",
      " |      +-------+---+-----+\n",
      " |      \n",
      " |      To do a summary for specific columns first select them:\n",
      " |      \n",
      " |      >>> df.select(\"age\", \"name\").summary(\"count\").show()\n",
      " |      +-------+---+----+\n",
      " |      |summary|age|name|\n",
      " |      +-------+---+----+\n",
      " |      |  count|  2|   2|\n",
      " |      +-------+---+----+\n",
      " |      \n",
      " |      See also describe for basic statistics.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |  \n",
      " |  tail(self, num)\n",
      " |      Returns the last ``num`` rows as a :class:`list` of :class:`Row`.\n",
      " |      \n",
      " |      Running tail requires moving data into the application's driver process, and doing so with\n",
      " |      a very large ``num`` can crash the driver process with OutOfMemoryError.\n",
      " |      \n",
      " |      >>> df.tail(1)\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      \n",
      " |      .. versionadded:: 3.0\n",
      " |  \n",
      " |  take(self, num)\n",
      " |      Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\n",
      " |      \n",
      " |      >>> df.take(2)\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  toDF(self, *cols)\n",
      " |      Returns a new :class:`DataFrame` that with new specified column names\n",
      " |      \n",
      " |      :param cols: list of new column names (string)\n",
      " |      \n",
      " |      >>> df.toDF('f1', 'f2').collect()\n",
      " |      [Row(f1=2, f2='Alice'), Row(f1=5, f2='Bob')]\n",
      " |  \n",
      " |  toJSON(self, use_unicode=True)\n",
      " |      Converts a :class:`DataFrame` into a :class:`RDD` of string.\n",
      " |      \n",
      " |      Each row is turned into a JSON document as one element in the returned RDD.\n",
      " |      \n",
      " |      >>> df.toJSON().first()\n",
      " |      '{\"age\":2,\"name\":\"Alice\"}'\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  toLocalIterator(self, prefetchPartitions=False)\n",
      " |      Returns an iterator that contains all of the rows in this :class:`DataFrame`.\n",
      " |      The iterator will consume as much memory as the largest partition in this\n",
      " |      :class:`DataFrame`. With prefetch it may consume up to the memory of the 2 largest\n",
      " |      partitions.\n",
      " |      \n",
      " |      :param prefetchPartitions: If Spark should pre-fetch the next partition\n",
      " |                                 before it is needed.\n",
      " |      \n",
      " |      >>> list(df.toLocalIterator())\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  transform(self, func)\n",
      " |      Returns a new :class:`DataFrame`. Concise syntax for chaining custom transformations.\n",
      " |      \n",
      " |      :param func: a function that takes and returns a :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> from pyspark.sql.functions import col\n",
      " |      >>> df = spark.createDataFrame([(1, 1.0), (2, 2.0)], [\"int\", \"float\"])\n",
      " |      >>> def cast_all_to_int(input_df):\n",
      " |      ...     return input_df.select([col(col_name).cast(\"int\") for col_name in input_df.columns])\n",
      " |      >>> def sort_columns_asc(input_df):\n",
      " |      ...     return input_df.select(*sorted(input_df.columns))\n",
      " |      >>> df.transform(cast_all_to_int).transform(sort_columns_asc).show()\n",
      " |      +-----+---+\n",
      " |      |float|int|\n",
      " |      +-----+---+\n",
      " |      |    1|  1|\n",
      " |      |    2|  2|\n",
      " |      +-----+---+\n",
      " |      \n",
      " |      .. versionadded:: 3.0\n",
      " |  \n",
      " |  union(self, other)\n",
      " |      Return a new :class:`DataFrame` containing union of rows in this and another\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n",
      " |      (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      " |      \n",
      " |      Also as standard in SQL, this function resolves columns by position (not by name).\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  unionAll(self, other)\n",
      " |      Return a new :class:`DataFrame` containing union of rows in this and another\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n",
      " |      (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      " |      \n",
      " |      Also as standard in SQL, this function resolves columns by position (not by name).\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  unionByName(self, other)\n",
      " |      Returns a new :class:`DataFrame` containing union of rows in this and another\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set\n",
      " |      union (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      " |      \n",
      " |      The difference between this function and :func:`union` is that this function\n",
      " |      resolves columns by name (not by position):\n",
      " |      \n",
      " |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
      " |      >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n",
      " |      >>> df1.unionByName(df2).show()\n",
      " |      +----+----+----+\n",
      " |      |col0|col1|col2|\n",
      " |      +----+----+----+\n",
      " |      |   1|   2|   3|\n",
      " |      |   6|   4|   5|\n",
      " |      +----+----+----+\n",
      " |      \n",
      " |      .. versionadded:: 2.3\n",
      " |  \n",
      " |  unpersist(self, blocking=False)\n",
      " |      Marks the :class:`DataFrame` as non-persistent, and remove all blocks for it from\n",
      " |      memory and disk.\n",
      " |      \n",
      " |      .. note:: `blocking` default has changed to ``False`` to match Scala in 2.0.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  where = filter(self, condition)\n",
      " |      :func:`where` is an alias for :func:`filter`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  withColumn(self, colName, col)\n",
      " |      Returns a new :class:`DataFrame` by adding a column or replacing the\n",
      " |      existing column that has the same name.\n",
      " |      \n",
      " |      The column expression must be an expression over this :class:`DataFrame`; attempting to add\n",
      " |      a column from some other :class:`DataFrame` will raise an error.\n",
      " |      \n",
      " |      :param colName: string, name of the new column.\n",
      " |      :param col: a :class:`Column` expression for the new column.\n",
      " |      \n",
      " |      .. note:: This method introduces a projection internally. Therefore, calling it multiple\n",
      " |          times, for instance, via loops in order to add multiple columns can generate big\n",
      " |          plans which can cause performance issues and even `StackOverflowException`.\n",
      " |          To avoid this, use :func:`select` with the multiple columns at once.\n",
      " |      \n",
      " |      >>> df.withColumn('age2', df.age + 2).collect()\n",
      " |      [Row(age=2, name='Alice', age2=4), Row(age=5, name='Bob', age2=7)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  withColumnRenamed(self, existing, new)\n",
      " |      Returns a new :class:`DataFrame` by renaming an existing column.\n",
      " |      This is a no-op if schema doesn't contain the given column name.\n",
      " |      \n",
      " |      :param existing: string, name of the existing column to rename.\n",
      " |      :param new: string, new name of the column.\n",
      " |      \n",
      " |      >>> df.withColumnRenamed('age', 'age2').collect()\n",
      " |      [Row(age2=2, name='Alice'), Row(age2=5, name='Bob')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  withWatermark(self, eventTime, delayThreshold)\n",
      " |      Defines an event time watermark for this :class:`DataFrame`. A watermark tracks a point\n",
      " |      in time before which we assume no more late data is going to arrive.\n",
      " |      \n",
      " |      Spark will use this watermark for several purposes:\n",
      " |        - To know when a given time window aggregation can be finalized and thus can be emitted\n",
      " |          when using output modes that do not allow updates.\n",
      " |      \n",
      " |        - To minimize the amount of state that we need to keep for on-going aggregations.\n",
      " |      \n",
      " |      The current watermark is computed by looking at the `MAX(eventTime)` seen across\n",
      " |      all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost\n",
      " |      of coordinating this value across partitions, the actual watermark used is only guaranteed\n",
      " |      to be at least `delayThreshold` behind the actual event time.  In some cases we may still\n",
      " |      process records that arrive more than `delayThreshold` late.\n",
      " |      \n",
      " |      :param eventTime: the name of the column that contains the event time of the row.\n",
      " |      :param delayThreshold: the minimum delay to wait to data to arrive late, relative to the\n",
      " |          latest record that has been processed in the form of an interval\n",
      " |          (e.g. \"1 minute\" or \"5 hours\").\n",
      " |      \n",
      " |      .. note:: Evolving\n",
      " |      \n",
      " |      >>> sdf.select('name', sdf.time.cast('timestamp')).withWatermark('time', '10 minutes')\n",
      " |      DataFrame[name: string, time: timestamp]\n",
      " |      \n",
      " |      .. versionadded:: 2.1\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  columns\n",
      " |      Returns all column names as a list.\n",
      " |      \n",
      " |      >>> df.columns\n",
      " |      ['age', 'name']\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  dtypes\n",
      " |      Returns all column names and their data types as a list.\n",
      " |      \n",
      " |      >>> df.dtypes\n",
      " |      [('age', 'int'), ('name', 'string')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  isStreaming\n",
      " |      Returns ``True`` if this :class:`Dataset` contains one or more sources that continuously\n",
      " |      return data as it arrives. A :class:`Dataset` that reads data from a streaming source\n",
      " |      must be executed as a :class:`StreamingQuery` using the :func:`start` method in\n",
      " |      :class:`DataStreamWriter`.  Methods that return a single answer, (e.g., :func:`count` or\n",
      " |      :func:`collect`) will throw an :class:`AnalysisException` when there is a streaming\n",
      " |      source present.\n",
      " |      \n",
      " |      .. note:: Evolving\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  na\n",
      " |      Returns a :class:`DataFrameNaFunctions` for handling missing values.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |  \n",
      " |  rdd\n",
      " |      Returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  schema\n",
      " |      Returns the schema of this :class:`DataFrame` as a :class:`pyspark.sql.types.StructType`.\n",
      " |      \n",
      " |      >>> df.schema\n",
      " |      StructType(List(StructField(age,IntegerType,true),StructField(name,StringType,true)))\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  stat\n",
      " |      Returns a :class:`DataFrameStatFunctions` for statistic functions.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  storageLevel\n",
      " |      Get the :class:`DataFrame`'s current storage level.\n",
      " |      \n",
      " |      >>> df.storageLevel\n",
      " |      StorageLevel(False, False, False, False, 1)\n",
      " |      >>> df.cache().storageLevel\n",
      " |      StorageLevel(True, True, False, True, 1)\n",
      " |      >>> df2.persist(StorageLevel.DISK_ONLY_2).storageLevel\n",
      " |      StorageLevel(True, False, False, False, 2)\n",
      " |      \n",
      " |      .. versionadded:: 2.1\n",
      " |  \n",
      " |  write\n",
      " |      Interface for saving the content of the non-streaming :class:`DataFrame` out into external\n",
      " |      storage.\n",
      " |      \n",
      " |      :return: :class:`DataFrameWriter`\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  writeStream\n",
      " |      Interface for saving the content of the streaming :class:`DataFrame` out into external\n",
      " |      storage.\n",
      " |      \n",
      " |      .. note:: Evolving.\n",
      " |      \n",
      " |      :return: :class:`DataStreamWriter`\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n",
      " |  \n",
      " |  mapInPandas(self, func, schema)\n",
      " |      Maps an iterator of batches in the current :class:`DataFrame` using a Python native\n",
      " |      function that takes and outputs a pandas DataFrame, and returns the result as a\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      The function should take an iterator of `pandas.DataFrame`\\s and return\n",
      " |      another iterator of `pandas.DataFrame`\\s. All columns are passed\n",
      " |      together as an iterator of `pandas.DataFrame`\\s to the function and the\n",
      " |      returned iterator of `pandas.DataFrame`\\s are combined as a :class:`DataFrame`.\n",
      " |      Each `pandas.DataFrame` size can be controlled by\n",
      " |      `spark.sql.execution.arrow.maxRecordsPerBatch`.\n",
      " |      \n",
      " |      :param func: a Python native function that takes an iterator of `pandas.DataFrame`\\s, and\n",
      " |          outputs an iterator of `pandas.DataFrame`\\s.\n",
      " |      :param schema: the return type of the `func` in PySpark. The value can be either a\n",
      " |          :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      " |      \n",
      " |      >>> from pyspark.sql.functions import pandas_udf\n",
      " |      >>> df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n",
      " |      >>> def filter_func(iterator):\n",
      " |      ...     for pdf in iterator:\n",
      " |      ...         yield pdf[pdf.id == 1]\n",
      " |      >>> df.mapInPandas(filter_func, df.schema).show()  # doctest: +SKIP\n",
      " |      +---+---+\n",
      " |      | id|age|\n",
      " |      +---+---+\n",
      " |      |  1| 21|\n",
      " |      +---+---+\n",
      " |      \n",
      " |      .. seealso:: :meth:`pyspark.sql.functions.pandas_udf`\n",
      " |      \n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      .. versionadded:: 3.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.sql.pandas.conversion.PandasConversionMixin:\n",
      " |  \n",
      " |  toPandas(self)\n",
      " |      Returns the contents of this :class:`DataFrame` as Pandas ``pandas.DataFrame``.\n",
      " |      \n",
      " |      This is only available if Pandas is installed and available.\n",
      " |      \n",
      " |      .. note:: This method should only be used if the resulting Pandas's :class:`DataFrame` is\n",
      " |          expected to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      .. note:: Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.\n",
      " |      \n",
      " |      >>> df.toPandas()  # doctest: +SKIP\n",
      " |         age   name\n",
      " |      0    2  Alice\n",
      " |      1    5    Bob\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(google_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------------+\n",
      "|year|month|          avg(Low)|\n",
      "+----+-----+------------------+\n",
      "|2019|    7|1948.1946176153847|\n",
      "|2019|    8|1778.2340920454546|\n",
      "|2019|    9|     1784.68599235|\n",
      "|2019|   10|1736.6047840869564|\n",
      "|2019|   11|1764.3005004499998|\n",
      "|2019|   12|1773.8466622380954|\n",
      "|2020|    1| 1870.354288809524|\n",
      "|2020|    2|2045.0563194210524|\n",
      "|2020|    3|1825.1590964090908|\n",
      "|2020|    4|2185.9347679523808|\n",
      "|2020|    5|     2364.48549805|\n",
      "|2020|    6| 2579.099553909091|\n",
      "|2020|    7|2976.1799859999996|\n",
      "+----+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "amazon_df.select(year(\"Date\").alias(\"year\"),\n",
    "                      month(\"Date\").alias(\"month\"), \n",
    "                      \"Low\") \\\n",
    "                 .groupby(\"year\",\"month\") \\\n",
    "                 .avg(\"Low\") \\\n",
    "                 .sort(\"year\",\"month\") \\\n",
    "                 .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['year ASC NULLS FIRST, 'month ASC NULLS FIRST], true\n",
      "+- Aggregate [year#196, month#197], [year#196, month#197, avg(Low#37) AS avg(Low)#205]\n",
      "   +- Project [year(cast(Date#34 as date)) AS year#196, month(cast(Date#34 as date)) AS month#197, Low#37]\n",
      "      +- Relation[Date#34,Open#35,High#36,Low#37,Close#38,AdjClose#39,Volume#40] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "year: int, month: int, avg(Low): double\n",
      "Sort [year#196 ASC NULLS FIRST, month#197 ASC NULLS FIRST], true\n",
      "+- Aggregate [year#196, month#197], [year#196, month#197, avg(Low#37) AS avg(Low)#205]\n",
      "   +- Project [year(cast(Date#34 as date)) AS year#196, month(cast(Date#34 as date)) AS month#197, Low#37]\n",
      "      +- Relation[Date#34,Open#35,High#36,Low#37,Close#38,AdjClose#39,Volume#40] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [year#196 ASC NULLS FIRST, month#197 ASC NULLS FIRST], true\n",
      "+- Aggregate [year#196, month#197], [year#196, month#197, avg(Low#37) AS avg(Low)#205]\n",
      "   +- Project [year(cast(Date#34 as date)) AS year#196, month(cast(Date#34 as date)) AS month#197, Low#37]\n",
      "      +- Relation[Date#34,Open#35,High#36,Low#37,Close#38,AdjClose#39,Volume#40] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(3) Sort [year#196 ASC NULLS FIRST, month#197 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(year#196 ASC NULLS FIRST, month#197 ASC NULLS FIRST, 200), true, [id=#173]\n",
      "   +- *(2) HashAggregate(keys=[year#196, month#197], functions=[avg(Low#37)], output=[year#196, month#197, avg(Low)#205])\n",
      "      +- Exchange hashpartitioning(year#196, month#197, 200), true, [id=#169]\n",
      "         +- *(1) HashAggregate(keys=[year#196, month#197], functions=[partial_avg(Low#37)], output=[year#196, month#197, sum#211, count#212L])\n",
      "            +- *(1) Project [year(cast(Date#34 as date)) AS year#196, month(cast(Date#34 as date)) AS month#197, Low#37]\n",
      "               +- FileScan csv [Date#34,Low#37] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/jovyan/work/test_automation/AMZN.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Date:string,Low:double>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "amazon_df.select(year(\"Date\").alias(\"year\"),\n",
    "                      month(\"Date\").alias(\"month\"), \n",
    "                      \"Low\") \\\n",
    "                 .groupby(\"year\",\"month\") \\\n",
    "                 .avg(\"Low\") \\\n",
    "                 .sort(\"year\",\"month\") \\\n",
    "                 .explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_df.registerTempTable(\"amazon_stocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_df.registerTempTable(\"google_stocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tesla_file = 'TSLA.csv'\n",
    "tesla_df = sqlContext.read.load(tesla_file,\n",
    "                                format='com.databricks.spark.csv',\n",
    "                                header='true',\n",
    "                                inferSchema='true')\n",
    "tesla_df.registerTempTable(\"tesla_stocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+-----------+-----------+-------+\n",
      "|      Date|       Open|       High|        Low|      Close|   AdjClose| Volume|\n",
      "+----------+-----------+-----------+-----------+-----------+-----------+-------+\n",
      "|2019-07-15|2021.400024|2022.900024|2001.550049| 2020.98999| 2020.98999|2981300|\n",
      "|2019-07-16|2010.579956|2026.319946|2001.219971|2009.900024|2009.900024|2618200|\n",
      "|2019-07-17|2007.050049|     2012.0|1992.030029|1992.030029|1992.030029|2558800|\n",
      "|2019-07-18| 1980.01001|     1987.5|1951.550049|1977.900024|1977.900024|3504300|\n",
      "|2019-07-19|1991.209961|     1996.0| 1962.22998| 1964.52002| 1964.52002|3185600|\n",
      "+----------+-----------+-----------+-----------+-----------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT * FROM amazon_stocks\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------------+\n",
      "|year|month|        avg(Close)|\n",
      "+----+-----+------------------+\n",
      "|2019|   10|1752.3317498695653|\n",
      "|2020|    6|      2613.5454545|\n",
      "|2020|    3|1872.3104358636365|\n",
      "|2019|    8|1793.6027220909093|\n",
      "|2020|    4|2228.7052408571426|\n",
      "|2020|    1|1884.2376128571425|\n",
      "|2019|    9|     1799.12099615|\n",
      "|2019|   12|1785.7728446190476|\n",
      "|2020|    7| 3053.100016222222|\n",
      "|2020|    2|2066.1752672631574|\n",
      "|2019|    7|1964.6846265384618|\n",
      "|2019|   11|      1774.2939941|\n",
      "|2020|    5|2394.1840209499996|\n",
      "+----+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT year(Date) as year, month(Date) as month, avg(Close) FROM amazon_stocks GROUP BY year, month\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+------------------+\n",
      "|      Date|       Open|      Close|               dif|\n",
      "+----------+-----------+-----------+------------------+\n",
      "|2019-07-16|     1146.0|1153.579956| 7.579956000000038|\n",
      "|2019-07-17|1150.969971|1146.349976| 4.619995000000017|\n",
      "|2019-07-18| 1141.73999|1146.329956| 4.589966000000004|\n",
      "|2019-07-19|1148.189941|1130.099976| 18.08996500000012|\n",
      "|2019-07-22|1133.449951|1138.069946| 4.619995000000017|\n",
      "|2019-07-24|1131.900024|1137.810059|  5.91003499999988|\n",
      "|2019-07-25|1137.819946|1132.119995|5.6999510000000555|\n",
      "|2019-07-26|1224.040039|1250.410034|26.369995000000017|\n",
      "|2019-07-31|     1223.0|1216.680054| 6.319946000000073|\n",
      "|2019-08-01|1214.030029| 1209.01001|5.0200190000000475|\n",
      "|2019-08-02| 1200.73999| 1193.98999|              6.75|\n",
      "|2019-08-05|1170.040039|1152.319946|17.720092999999906|\n",
      "|2019-08-06|1163.310059|1169.949951| 6.639892000000145|\n",
      "|2019-08-07|     1156.0| 1173.98999|17.989990000000034|\n",
      "|2019-08-08|1182.829956|1204.800049|21.970092999999906|\n",
      "|2019-08-09| 1197.98999| 1188.01001| 9.979980000000069|\n",
      "|2019-08-12|1179.209961|1174.709961|               4.5|\n",
      "|2019-08-13|1171.459961| 1197.27002| 25.81005899999991|\n",
      "|2019-08-14|1176.310059|1164.290039|12.020019999999931|\n",
      "|2019-08-19|1190.089966|1198.449951| 8.359985000000052|\n",
      "+----------+-----------+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# When did the closing price of Google go either up or down by more than $4 in a single day?\n",
    "sqlContext.sql(\"SELECT Date, Open, Close, abs(Close - Open) as dif FROM google_stocks WHERE abs(Close - Open) > 4\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-----------+\n",
      "|year|       min|        max|\n",
      "+----+----------+-----------+\n",
      "|2019|211.399994| 430.940002|\n",
      "|2020|361.220001|1544.650024|\n",
      "+----+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Min and Max Adjusted Close price per year for Tesla\n",
    "sqlContext.sql(\"SELECT year(Date) as year, \\\n",
    "                min(AdjClose) as min, max(AdjClose) as max \\\n",
    "                FROM tesla_stocks \\\n",
    "                GROUP BY year\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinclose_df = sqlContext.sql(\"SELECT tesla_stocks.Date, \\\n",
    "                tesla_stocks.Close as teslaClose, \\\n",
    "                amazon_stocks.Close as amazonClose, \\\n",
    "                google_stocks.Close as googleClose \\\n",
    "                FROM tesla_stocks \\\n",
    "                JOIN google_stocks ON tesla_stocks.Date = google_stocks.Date \\\n",
    "                JOIN amazon_stocks ON amazon_stocks.Date = tesla_stocks.Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+-----------+\n",
      "|      Date|teslaClose|amazonClose|googleClose|\n",
      "+----------+----------+-----------+-----------+\n",
      "|2019-07-15|     253.5| 2020.98999|1150.339966|\n",
      "|2019-07-16|252.380005|2009.900024|1153.579956|\n",
      "|2019-07-17|254.860001|1992.030029|1146.349976|\n",
      "|2019-07-18|253.539993|1977.900024|1146.329956|\n",
      "|2019-07-19|258.179993| 1964.52002|1130.099976|\n",
      "|2019-07-22|255.679993|1985.630005|1138.069946|\n",
      "|2019-07-23|260.170013| 1994.48999|1146.209961|\n",
      "|2019-07-24|264.880005|2000.810059|1137.810059|\n",
      "|2019-07-25|228.820007|1973.819946|1132.119995|\n",
      "|2019-07-26|228.039993|1943.050049|1250.410034|\n",
      "|2019-07-29|235.770004|1912.449951|1239.410034|\n",
      "|2019-07-30|242.259995|1898.530029|1225.140015|\n",
      "|2019-07-31|241.610001|1866.780029|1216.680054|\n",
      "|2019-08-01|233.850006|1855.319946| 1209.01001|\n",
      "|2019-08-02|234.339996| 1823.23999| 1193.98999|\n",
      "|2019-08-05|228.320007|1765.130005|1152.319946|\n",
      "|2019-08-06|    230.75|1787.829956|1169.949951|\n",
      "|2019-08-07|233.419998|1793.400024| 1173.98999|\n",
      "|2019-08-08|238.300003|1832.890015|1204.800049|\n",
      "|2019-08-09|235.009995|1807.579956| 1188.01001|\n",
      "+----------+----------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinclose_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------------+------------------+------------------+\n",
      "|year|month|     AvgTeslaClose|    AvgAmazonClose|    AvgGoogleClose|\n",
      "+----+-----+------------------+------------------+------------------+\n",
      "|2019|   10| 266.3547840434783|1752.3317498695653|1232.7117442608696|\n",
      "|2020|    6| 963.5422779545456|      2613.5454545|1431.0477184545452|\n",
      "|2020|    3| 559.1013613181818|1872.3104358636365|1188.3940984545457|\n",
      "|2019|    8|225.10272704545451|1793.6027220909093|1180.6868120454546|\n",
      "|2020|    4| 663.5985761428572|2228.7052408571426|1234.1404797142854|\n",
      "|2020|    1| 528.6590503809524|1884.2376128571425|1436.6537968571424|\n",
      "|2019|    9|237.26149830000003|     1799.12099615|     1220.83952035|\n",
      "|2019|   12| 377.6947631904762|1785.7728446190476|1340.8676351904762|\n",
      "|2020|    7|1378.7111273333333| 3053.100016222222|1496.0299885555555|\n",
      "|2020|    2| 797.4468415263159|2066.1752672631574|1464.1105184736841|\n",
      "|2019|    7|248.43769253846148|1964.6846265384618|1170.1961483076923|\n",
      "|2019|   11|338.30000000000007|      1774.2939941|     1304.27899165|\n",
      "|2020|    5|      799.42549745|2394.1840209499996|1381.1137511999998|\n",
      "+----+-----+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_close_df = sqlContext.sql(\"SELECT year(tesla_stocks.Date) as year, \\\n",
    "                month(tesla_stocks.Date) as month, \\\n",
    "                avg(tesla_stocks.Close) as AvgTeslaClose, \\\n",
    "                avg(amazon_stocks.Close) as AvgAmazonClose, \\\n",
    "                avg(google_stocks.Close) as AvgGoogleClose \\\n",
    "                FROM tesla_stocks \\\n",
    "                JOIN google_stocks ON tesla_stocks.Date = google_stocks.Date \\\n",
    "                JOIN amazon_stocks ON amazon_stocks.Date = tesla_stocks.Date \\\n",
    "                GROUP BY year, month\")\n",
    "avg_close_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Spark DataFrames\n",
    "## Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinclose_df.write.format(\"parquet\").save(\"joinStock.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = sqlContext.read.parquet(\"joinStock.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+-----------+\n",
      "|      Date|teslaClose|amazonClose|googleClose|\n",
      "+----------+----------+-----------+-----------+\n",
      "|2019-07-15|     253.5| 2020.98999|1150.339966|\n",
      "|2019-07-16|252.380005|2009.900024|1153.579956|\n",
      "|2019-07-17|254.860001|1992.030029|1146.349976|\n",
      "|2019-07-18|253.539993|1977.900024|1146.329956|\n",
      "|2019-07-19|258.179993| 1964.52002|1130.099976|\n",
      "|2019-07-22|255.679993|1985.630005|1138.069946|\n",
      "|2019-07-23|260.170013| 1994.48999|1146.209961|\n",
      "|2019-07-24|264.880005|2000.810059|1137.810059|\n",
      "|2019-07-25|228.820007|1973.819946|1132.119995|\n",
      "|2019-07-26|228.039993|1943.050049|1250.410034|\n",
      "|2019-07-29|235.770004|1912.449951|1239.410034|\n",
      "|2019-07-30|242.259995|1898.530029|1225.140015|\n",
      "|2019-07-31|241.610001|1866.780029|1216.680054|\n",
      "|2019-08-01|233.850006|1855.319946| 1209.01001|\n",
      "|2019-08-02|234.339996| 1823.23999| 1193.98999|\n",
      "|2019-08-05|228.320007|1765.130005|1152.319946|\n",
      "|2019-08-06|    230.75|1787.829956|1169.949951|\n",
      "|2019-08-07|233.419998|1793.400024| 1173.98999|\n",
      "|2019-08-08|238.300003|1832.890015|1204.800049|\n",
      "|2019-08-09|235.009995|1807.579956| 1188.01001|\n",
      "+----------+----------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- teslaClose: double (nullable = true)\n",
      " |-- amazonClose: double (nullable = true)\n",
      " |-- googleClose: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
